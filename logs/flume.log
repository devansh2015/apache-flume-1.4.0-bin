06 Dec 2014 11:00:21,800 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
06 Dec 2014 11:00:21,807 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
06 Dec 2014 11:00:21,813 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:00:21,813 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:00:21,813 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfsSink Agent: agent
06 Dec 2014 11:00:21,814 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:00:21,814 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:00:21,814 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:00:21,834 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
06 Dec 2014 11:00:21,834 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
06 Dec 2014 11:00:21,849 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memoryChannel type memory
06 Dec 2014 11:00:21,860 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memoryChannel
06 Dec 2014 11:00:21,864 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source pstream, type exec
06 Dec 2014 11:00:21,877 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfsSink, type: hdfs
06 Dec 2014 11:00:22,300 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
06 Dec 2014 11:00:22,654 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
06 Dec 2014 11:00:22,662 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memoryChannel connected to [pstream, hdfsSink]
06 Dec 2014 11:00:22,678 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{pstream=EventDrivenSourceRunner: { source:org.apache.flume.source.ExecSource{name:pstream,state:IDLE} }} sinkRunners:{hdfsSink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@5a68c1b6 counterGroup:{ name:null counters:{} } }} channels:{memoryChannel=org.apache.flume.channel.MemoryChannel{name: memoryChannel}} }
06 Dec 2014 11:00:22,689 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memoryChannel
06 Dec 2014 11:00:22,734 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memoryChannel, registered successfully.
06 Dec 2014 11:00:22,734 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memoryChannel started
06 Dec 2014 11:00:22,734 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfsSink
06 Dec 2014 11:00:22,735 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source pstream
06 Dec 2014 11:00:22,735 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.ExecSource.start:163)  - Exec source starting with command:tail -f /etc/passwd
06 Dec 2014 11:00:22,738 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfsSink, registered successfully.
06 Dec 2014 11:00:22,739 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfsSink started
06 Dec 2014 11:00:22,741 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SOURCE, name: pstream, registered successfully.
06 Dec 2014 11:00:22,742 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SOURCE, name: pstream started
06 Dec 2014 11:00:26,748 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSSequenceFile.configure:63)  - writeFormat = Text, UseRawLocalFileSystem = false
06 Dec 2014 11:00:26,892 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417843826747.tmp
06 Dec 2014 11:00:27,406 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:422)  - process failed
java.lang.VerifyError: class org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto overrides final method getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2531)
	at java.lang.Class.privateGetPublicMethods(Class.java:2651)
	at java.lang.Class.privateGetPublicMethods(Class.java:2661)
	at java.lang.Class.getMethods(Class.java:1467)
	at sun.misc.ProxyGenerator.generateClassFile(ProxyGenerator.java:426)
	at sun.misc.ProxyGenerator.generateProxyClass(ProxyGenerator.java:323)
	at java.lang.reflect.Proxy.getProxyClass0(Proxy.java:636)
	at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:722)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy(ProtobufRpcEngine.java:92)
	at org.apache.hadoop.ipc.RPC.getProtocolProxy(RPC.java:537)
	at org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(NameNodeProxies.java:348)
	at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:244)
	at org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider.getProxy(ConfiguredFailoverProxyProvider.java:124)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.<init>(RetryInvocationHandler.java:74)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.<init>(RetryInvocationHandler.java:65)
	at org.apache.hadoop.io.retry.RetryProxy.create(RetryProxy.java:58)
	at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:152)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:579)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:524)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:146)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2397)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:226)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
06 Dec 2014 11:02:15,118 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:79)  - Stopping lifecycle supervisor 10
06 Dec 2014 11:02:15,121 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:437)  - Closing hdfs://hacluster:8020/flumetest/FlumeData
06 Dec 2014 11:02:15,122 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.close:296)  - HDFSWriter is already closed: hdfs://hacluster:8020/flumetest/FlumeData.1417843826747.tmp
06 Dec 2014 11:02:15,123 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfsSink stopped
06 Dec 2014 11:02:15,123 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.start.time == 1417843822739
06 Dec 2014 11:02:15,123 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.stop.time == 1417843935123
06 Dec 2014 11:02:15,124 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.complete == 0
06 Dec 2014 11:02:15,124 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.empty == 1
06 Dec 2014 11:02:15,124 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.underflow == 0
06 Dec 2014 11:02:15,124 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.closed.count == 0
06 Dec 2014 11:02:15,124 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.creation.count == 0
06 Dec 2014 11:02:15,124 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.failed.count == 1
06 Dec 2014 11:02:15,124 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.attempt == 0
06 Dec 2014 11:02:15,124 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.sucess == 0
06 Dec 2014 11:02:15,124 INFO  [agent-shutdown-hook] (org.apache.flume.source.ExecSource.stop:186)  - Stopping exec source with command:tail -f /etc/passwd
06 Dec 2014 11:02:15,126 INFO  [pool-3-thread-1] (org.apache.flume.source.ExecSource$ExecRunnable.run:370)  - Command [tail -f /etc/passwd] exited with 143
06 Dec 2014 11:02:15,126 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SOURCE, name: pstream stopped
06 Dec 2014 11:02:15,126 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SOURCE, name: pstream. source.start.time == 1417843822742
06 Dec 2014 11:02:15,126 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SOURCE, name: pstream. source.stop.time == 1417843935126
06 Dec 2014 11:02:15,126 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.append-batch.accepted == 0
06 Dec 2014 11:02:15,126 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.append-batch.received == 0
06 Dec 2014 11:02:15,127 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.append.accepted == 0
06 Dec 2014 11:02:15,127 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.append.received == 0
06 Dec 2014 11:02:15,127 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.events.accepted == 10
06 Dec 2014 11:02:15,127 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.events.received == 10
06 Dec 2014 11:02:15,127 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.open-connection.count == 0
06 Dec 2014 11:02:15,127 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memoryChannel stopped
06 Dec 2014 11:02:15,127 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.start.time == 1417843822734
06 Dec 2014 11:02:15,127 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.stop.time == 1417843935127
06 Dec 2014 11:02:15,127 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.capacity == 100
06 Dec 2014 11:02:15,127 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.current.size == 10
06 Dec 2014 11:02:15,127 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.attempt == 10
06 Dec 2014 11:02:15,127 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.success == 10
06 Dec 2014 11:02:15,128 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.attempt == 2
06 Dec 2014 11:02:15,128 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.success == 0
06 Dec 2014 11:02:15,128 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:83)  - Configuration provider stopping
06 Dec 2014 11:02:17,206 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
06 Dec 2014 11:02:17,212 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
06 Dec 2014 11:02:17,217 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:02:17,218 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:02:17,218 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfsSink Agent: agent
06 Dec 2014 11:02:17,218 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:02:17,218 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:02:17,218 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:02:17,243 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
06 Dec 2014 11:02:17,243 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
06 Dec 2014 11:02:17,258 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memoryChannel type memory
06 Dec 2014 11:02:17,265 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memoryChannel
06 Dec 2014 11:02:17,266 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source pstream, type exec
06 Dec 2014 11:02:17,281 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfsSink, type: hdfs
06 Dec 2014 11:02:17,722 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
06 Dec 2014 11:02:18,059 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
06 Dec 2014 11:02:18,072 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memoryChannel connected to [pstream, hdfsSink]
06 Dec 2014 11:02:18,083 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{pstream=EventDrivenSourceRunner: { source:org.apache.flume.source.ExecSource{name:pstream,state:IDLE} }} sinkRunners:{hdfsSink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@5a68c1b6 counterGroup:{ name:null counters:{} } }} channels:{memoryChannel=org.apache.flume.channel.MemoryChannel{name: memoryChannel}} }
06 Dec 2014 11:02:18,098 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memoryChannel
06 Dec 2014 11:02:18,143 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memoryChannel, registered successfully.
06 Dec 2014 11:02:18,144 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memoryChannel started
06 Dec 2014 11:02:18,144 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfsSink
06 Dec 2014 11:02:18,144 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source pstream
06 Dec 2014 11:02:18,145 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.ExecSource.start:163)  - Exec source starting with command:tail -f /etc/passwd
06 Dec 2014 11:02:18,147 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfsSink, registered successfully.
06 Dec 2014 11:02:18,148 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfsSink started
06 Dec 2014 11:02:18,150 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SOURCE, name: pstream, registered successfully.
06 Dec 2014 11:02:18,150 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SOURCE, name: pstream started
06 Dec 2014 11:02:22,157 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSSequenceFile.configure:63)  - writeFormat = Text, UseRawLocalFileSystem = false
06 Dec 2014 11:02:22,288 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417843942156.tmp
06 Dec 2014 11:02:22,810 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:422)  - process failed
java.lang.VerifyError: class org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto overrides final method getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2531)
	at java.lang.Class.privateGetPublicMethods(Class.java:2651)
	at java.lang.Class.privateGetPublicMethods(Class.java:2661)
	at java.lang.Class.getMethods(Class.java:1467)
	at sun.misc.ProxyGenerator.generateClassFile(ProxyGenerator.java:426)
	at sun.misc.ProxyGenerator.generateProxyClass(ProxyGenerator.java:323)
	at java.lang.reflect.Proxy.getProxyClass0(Proxy.java:636)
	at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:722)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getProxy(ProtobufRpcEngine.java:92)
	at org.apache.hadoop.ipc.RPC.getProtocolProxy(RPC.java:537)
	at org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(NameNodeProxies.java:348)
	at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:244)
	at org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider.getProxy(ConfiguredFailoverProxyProvider.java:124)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.<init>(RetryInvocationHandler.java:74)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.<init>(RetryInvocationHandler.java:65)
	at org.apache.hadoop.io.retry.RetryProxy.create(RetryProxy.java:58)
	at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:152)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:579)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:524)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:146)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2397)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:226)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
06 Dec 2014 11:05:59,572 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:79)  - Stopping lifecycle supervisor 10
06 Dec 2014 11:05:59,573 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:437)  - Closing hdfs://hacluster:8020/flumetest/FlumeData
06 Dec 2014 11:05:59,573 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.close:296)  - HDFSWriter is already closed: hdfs://hacluster:8020/flumetest/FlumeData.1417843942156.tmp
06 Dec 2014 11:05:59,574 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfsSink stopped
06 Dec 2014 11:05:59,574 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.start.time == 1417843938148
06 Dec 2014 11:05:59,575 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.stop.time == 1417844159574
06 Dec 2014 11:05:59,575 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.complete == 0
06 Dec 2014 11:05:59,575 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.empty == 1
06 Dec 2014 11:05:59,575 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.underflow == 0
06 Dec 2014 11:05:59,575 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.closed.count == 0
06 Dec 2014 11:05:59,575 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.creation.count == 0
06 Dec 2014 11:05:59,575 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.failed.count == 1
06 Dec 2014 11:05:59,575 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.attempt == 0
06 Dec 2014 11:05:59,576 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.sucess == 0
06 Dec 2014 11:05:59,576 INFO  [agent-shutdown-hook] (org.apache.flume.source.ExecSource.stop:186)  - Stopping exec source with command:tail -f /etc/passwd
06 Dec 2014 11:05:59,576 INFO  [pool-3-thread-1] (org.apache.flume.source.ExecSource$ExecRunnable.run:370)  - Command [tail -f /etc/passwd] exited with 143
06 Dec 2014 11:05:59,577 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SOURCE, name: pstream stopped
06 Dec 2014 11:05:59,577 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SOURCE, name: pstream. source.start.time == 1417843938150
06 Dec 2014 11:05:59,577 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SOURCE, name: pstream. source.stop.time == 1417844159577
06 Dec 2014 11:05:59,577 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.append-batch.accepted == 0
06 Dec 2014 11:05:59,577 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.append-batch.received == 0
06 Dec 2014 11:05:59,577 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.append.accepted == 0
06 Dec 2014 11:05:59,577 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.append.received == 0
06 Dec 2014 11:05:59,577 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.events.accepted == 10
06 Dec 2014 11:05:59,577 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.events.received == 10
06 Dec 2014 11:05:59,577 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.open-connection.count == 0
06 Dec 2014 11:05:59,577 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memoryChannel stopped
06 Dec 2014 11:05:59,577 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.start.time == 1417843938144
06 Dec 2014 11:05:59,577 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.stop.time == 1417844159577
06 Dec 2014 11:05:59,577 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.capacity == 100
06 Dec 2014 11:05:59,578 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.current.size == 10
06 Dec 2014 11:05:59,578 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.attempt == 10
06 Dec 2014 11:05:59,578 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.success == 10
06 Dec 2014 11:05:59,578 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.attempt == 2
06 Dec 2014 11:05:59,578 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.success == 0
06 Dec 2014 11:05:59,578 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:83)  - Configuration provider stopping
06 Dec 2014 11:06:01,633 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
06 Dec 2014 11:06:01,639 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
06 Dec 2014 11:06:01,644 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:06:01,645 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:06:01,645 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfsSink Agent: agent
06 Dec 2014 11:06:01,646 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:06:01,646 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:06:01,646 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:06:01,662 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
06 Dec 2014 11:06:01,663 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
06 Dec 2014 11:06:01,672 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memoryChannel type memory
06 Dec 2014 11:06:01,680 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memoryChannel
06 Dec 2014 11:06:01,685 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source pstream, type exec
06 Dec 2014 11:06:01,696 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfsSink, type: hdfs
06 Dec 2014 11:06:02,125 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
06 Dec 2014 11:06:02,484 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
06 Dec 2014 11:06:02,492 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memoryChannel connected to [pstream, hdfsSink]
06 Dec 2014 11:06:02,520 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{pstream=EventDrivenSourceRunner: { source:org.apache.flume.source.ExecSource{name:pstream,state:IDLE} }} sinkRunners:{hdfsSink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2366c122 counterGroup:{ name:null counters:{} } }} channels:{memoryChannel=org.apache.flume.channel.MemoryChannel{name: memoryChannel}} }
06 Dec 2014 11:06:02,535 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memoryChannel
06 Dec 2014 11:06:02,603 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memoryChannel, registered successfully.
06 Dec 2014 11:06:02,604 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memoryChannel started
06 Dec 2014 11:06:02,604 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfsSink
06 Dec 2014 11:06:02,604 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source pstream
06 Dec 2014 11:06:02,605 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.ExecSource.start:163)  - Exec source starting with command:tail -f /etc/passwd
06 Dec 2014 11:06:02,614 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfsSink, registered successfully.
06 Dec 2014 11:06:02,614 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfsSink started
06 Dec 2014 11:06:02,615 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SOURCE, name: pstream, registered successfully.
06 Dec 2014 11:06:02,615 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SOURCE, name: pstream started
06 Dec 2014 11:06:06,627 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSSequenceFile.configure:63)  - writeFormat = Text, UseRawLocalFileSystem = false
06 Dec 2014 11:06:06,780 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417844166626.tmp
06 Dec 2014 11:06:38,602 INFO  [hdfs-hdfsSink-call-runner-3] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1417844166626.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1417844166626
06 Dec 2014 11:09:09,328 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:79)  - Stopping lifecycle supervisor 10
06 Dec 2014 11:09:09,333 INFO  [agent-shutdown-hook] (org.apache.flume.source.ExecSource.stop:186)  - Stopping exec source with command:tail -f /etc/passwd
06 Dec 2014 11:09:09,333 INFO  [pool-3-thread-1] (org.apache.flume.source.ExecSource$ExecRunnable.run:370)  - Command [tail -f /etc/passwd] exited with 143
06 Dec 2014 11:09:09,334 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SOURCE, name: pstream stopped
06 Dec 2014 11:09:09,334 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SOURCE, name: pstream. source.start.time == 1417844162615
06 Dec 2014 11:09:09,334 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SOURCE, name: pstream. source.stop.time == 1417844349334
06 Dec 2014 11:09:09,334 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.append-batch.accepted == 0
06 Dec 2014 11:09:09,334 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.append-batch.received == 0
06 Dec 2014 11:09:09,335 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.append.accepted == 0
06 Dec 2014 11:09:09,335 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.append.received == 0
06 Dec 2014 11:09:09,335 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.events.accepted == 10
06 Dec 2014 11:09:09,335 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.events.received == 10
06 Dec 2014 11:09:09,335 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: pstream. src.open-connection.count == 0
06 Dec 2014 11:09:09,335 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memoryChannel stopped
06 Dec 2014 11:09:09,335 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.start.time == 1417844162604
06 Dec 2014 11:09:09,335 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.stop.time == 1417844349335
06 Dec 2014 11:09:09,335 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.capacity == 100
06 Dec 2014 11:09:09,336 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.current.size == 0
06 Dec 2014 11:09:09,336 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.attempt == 10
06 Dec 2014 11:09:09,336 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.success == 10
06 Dec 2014 11:09:09,336 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.attempt == 36
06 Dec 2014 11:09:09,336 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.success == 10
06 Dec 2014 11:09:09,339 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:83)  - Configuration provider stopping
06 Dec 2014 11:09:09,339 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:437)  - Closing hdfs://hacluster:8020/flumetest/FlumeData
06 Dec 2014 11:09:09,339 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.close:296)  - HDFSWriter is already closed: hdfs://hacluster:8020/flumetest/FlumeData.1417844166626.tmp
06 Dec 2014 11:09:09,344 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfsSink stopped
06 Dec 2014 11:09:09,344 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.start.time == 1417844162614
06 Dec 2014 11:09:09,344 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.stop.time == 1417844349344
06 Dec 2014 11:09:09,345 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.complete == 0
06 Dec 2014 11:09:09,345 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.empty == 25
06 Dec 2014 11:09:09,345 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.underflow == 1
06 Dec 2014 11:09:09,345 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.closed.count == 1
06 Dec 2014 11:09:09,345 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.creation.count == 1
06 Dec 2014 11:09:09,345 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.failed.count == 0
06 Dec 2014 11:09:09,345 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.attempt == 10
06 Dec 2014 11:09:09,345 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.sucess == 10
06 Dec 2014 11:09:11,417 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
06 Dec 2014 11:09:11,423 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
06 Dec 2014 11:09:11,428 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:09:11,429 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:09:11,429 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfsSink Agent: agent
06 Dec 2014 11:09:11,429 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:09:11,429 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:09:11,429 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:09:11,446 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
06 Dec 2014 11:09:11,446 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
06 Dec 2014 11:09:11,456 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memoryChannel type memory
06 Dec 2014 11:09:11,465 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memoryChannel
06 Dec 2014 11:09:11,466 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source logstream, type exec
06 Dec 2014 11:09:11,480 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfsSink, type: hdfs
06 Dec 2014 11:09:11,883 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
06 Dec 2014 11:09:12,229 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
06 Dec 2014 11:09:12,233 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memoryChannel connected to [logstream, hdfsSink]
06 Dec 2014 11:09:12,264 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{logstream=EventDrivenSourceRunner: { source:org.apache.flume.source.ExecSource{name:logstream,state:IDLE} }} sinkRunners:{hdfsSink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2366c122 counterGroup:{ name:null counters:{} } }} channels:{memoryChannel=org.apache.flume.channel.MemoryChannel{name: memoryChannel}} }
06 Dec 2014 11:09:12,284 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memoryChannel
06 Dec 2014 11:09:12,373 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memoryChannel, registered successfully.
06 Dec 2014 11:09:12,376 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memoryChannel started
06 Dec 2014 11:09:12,376 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfsSink
06 Dec 2014 11:09:12,376 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source logstream
06 Dec 2014 11:09:12,377 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.ExecSource.start:163)  - Exec source starting with command:tail -f /var/log/messages
06 Dec 2014 11:09:12,387 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfsSink, registered successfully.
06 Dec 2014 11:09:12,388 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfsSink started
06 Dec 2014 11:09:12,390 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SOURCE, name: logstream, registered successfully.
06 Dec 2014 11:09:12,390 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SOURCE, name: logstream started
06 Dec 2014 11:09:12,395 INFO  [pool-3-thread-1] (org.apache.flume.source.ExecSource$ExecRunnable.run:370)  - Command [tail -f /var/log/messages] exited with 1
06 Dec 2014 11:13:19,739 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:79)  - Stopping lifecycle supervisor 10
06 Dec 2014 11:13:19,743 INFO  [agent-shutdown-hook] (org.apache.flume.source.ExecSource.stop:186)  - Stopping exec source with command:tail -f /var/log/messages
06 Dec 2014 11:13:19,744 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SOURCE, name: logstream stopped
06 Dec 2014 11:13:19,744 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SOURCE, name: logstream. source.start.time == 1417844352390
06 Dec 2014 11:13:19,744 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SOURCE, name: logstream. source.stop.time == 1417844599744
06 Dec 2014 11:13:19,744 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append-batch.accepted == 0
06 Dec 2014 11:13:19,744 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append-batch.received == 0
06 Dec 2014 11:13:19,744 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append.accepted == 0
06 Dec 2014 11:13:19,745 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append.received == 0
06 Dec 2014 11:13:19,745 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.events.accepted == 0
06 Dec 2014 11:13:19,745 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.events.received == 0
06 Dec 2014 11:13:19,745 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.open-connection.count == 0
06 Dec 2014 11:13:19,745 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memoryChannel stopped
06 Dec 2014 11:13:19,745 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.start.time == 1417844352376
06 Dec 2014 11:13:19,745 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.stop.time == 1417844599745
06 Dec 2014 11:13:19,745 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.capacity == 100
06 Dec 2014 11:13:19,745 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.current.size == 0
06 Dec 2014 11:13:19,745 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.attempt == 0
06 Dec 2014 11:13:19,746 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.success == 0
06 Dec 2014 11:13:19,746 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.attempt == 33
06 Dec 2014 11:13:19,746 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.success == 0
06 Dec 2014 11:13:19,746 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:83)  - Configuration provider stopping
06 Dec 2014 11:13:19,746 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfsSink stopped
06 Dec 2014 11:13:19,747 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.start.time == 1417844352388
06 Dec 2014 11:13:19,747 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.stop.time == 1417844599746
06 Dec 2014 11:13:19,753 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.complete == 0
06 Dec 2014 11:13:19,753 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.empty == 33
06 Dec 2014 11:13:19,753 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.underflow == 0
06 Dec 2014 11:13:19,753 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.closed.count == 0
06 Dec 2014 11:13:19,753 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.creation.count == 0
06 Dec 2014 11:13:19,753 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.failed.count == 0
06 Dec 2014 11:13:19,753 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.attempt == 0
06 Dec 2014 11:13:19,753 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.sucess == 0
06 Dec 2014 11:15:26,689 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
06 Dec 2014 11:15:26,695 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
06 Dec 2014 11:15:26,701 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:15:26,701 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:15:26,702 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfsSink Agent: agent
06 Dec 2014 11:15:26,702 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:15:26,702 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:15:26,702 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:15:26,719 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
06 Dec 2014 11:15:26,719 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
06 Dec 2014 11:15:26,730 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memoryChannel type memory
06 Dec 2014 11:15:26,737 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memoryChannel
06 Dec 2014 11:15:26,738 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source logstream, type exec
06 Dec 2014 11:15:26,752 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfsSink, type: hdfs
06 Dec 2014 11:15:27,143 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
06 Dec 2014 11:15:27,524 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
06 Dec 2014 11:15:27,533 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memoryChannel connected to [logstream, hdfsSink]
06 Dec 2014 11:15:27,560 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{logstream=EventDrivenSourceRunner: { source:org.apache.flume.source.ExecSource{name:logstream,state:IDLE} }} sinkRunners:{hdfsSink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2366c122 counterGroup:{ name:null counters:{} } }} channels:{memoryChannel=org.apache.flume.channel.MemoryChannel{name: memoryChannel}} }
06 Dec 2014 11:15:27,574 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memoryChannel
06 Dec 2014 11:15:27,653 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memoryChannel, registered successfully.
06 Dec 2014 11:15:27,654 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memoryChannel started
06 Dec 2014 11:15:27,654 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfsSink
06 Dec 2014 11:15:27,654 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source logstream
06 Dec 2014 11:15:27,655 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.ExecSource.start:163)  - Exec source starting with command:tail -f /apache/flume/logs/flume.log
06 Dec 2014 11:15:27,658 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfsSink, registered successfully.
06 Dec 2014 11:15:27,658 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfsSink started
06 Dec 2014 11:15:27,663 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SOURCE, name: logstream, registered successfully.
06 Dec 2014 11:15:27,663 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SOURCE, name: logstream started
06 Dec 2014 11:15:31,674 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSSequenceFile.configure:63)  - writeFormat = Text, UseRawLocalFileSystem = false
06 Dec 2014 11:15:31,803 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417844731673.tmp
06 Dec 2014 11:15:33,942 INFO  [hdfs-hdfsSink-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1417844731673.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1417844731673
06 Dec 2014 11:15:33,996 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417844731674.tmp
06 Dec 2014 11:15:36,741 INFO  [hdfs-hdfsSink-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1417844731674.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1417844731674
06 Dec 2014 11:15:36,799 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417844731675.tmp
06 Dec 2014 11:16:06,888 INFO  [hdfs-hdfsSink-call-runner-7] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1417844731675.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1417844731675
06 Dec 2014 11:16:06,940 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417844731676.tmp
06 Dec 2014 11:16:37,010 INFO  [hdfs-hdfsSink-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1417844731676.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1417844731676
06 Dec 2014 11:16:37,720 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417844731677.tmp
06 Dec 2014 11:17:07,795 INFO  [hdfs-hdfsSink-call-runner-1] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1417844731677.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1417844731677
06 Dec 2014 11:17:07,823 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417844731678.tmp
06 Dec 2014 11:17:37,892 INFO  [hdfs-hdfsSink-call-runner-8] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1417844731678.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1417844731678
06 Dec 2014 11:17:37,927 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417844731679.tmp
06 Dec 2014 11:17:41,437 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:79)  - Stopping lifecycle supervisor 10
06 Dec 2014 11:17:41,442 INFO  [agent-shutdown-hook] (org.apache.flume.source.ExecSource.stop:186)  - Stopping exec source with command:tail -f /apache/flume/logs/flume.log
06 Dec 2014 11:17:41,443 INFO  [pool-3-thread-1] (org.apache.flume.source.ExecSource$ExecRunnable.run:370)  - Command [tail -f /apache/flume/logs/flume.log] exited with 143
06 Dec 2014 11:17:41,444 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SOURCE, name: logstream stopped
06 Dec 2014 11:17:41,444 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SOURCE, name: logstream. source.start.time == 1417844727663
06 Dec 2014 11:17:41,444 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SOURCE, name: logstream. source.stop.time == 1417844861444
06 Dec 2014 11:17:41,444 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append-batch.accepted == 0
06 Dec 2014 11:17:41,444 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append-batch.received == 0
06 Dec 2014 11:17:41,449 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append.accepted == 0
06 Dec 2014 11:17:41,449 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append.received == 0
06 Dec 2014 11:17:41,449 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.events.accepted == 26
06 Dec 2014 11:17:41,450 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.events.received == 26
06 Dec 2014 11:17:41,450 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.open-connection.count == 0
06 Dec 2014 11:17:41,450 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memoryChannel stopped
06 Dec 2014 11:17:41,450 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.start.time == 1417844727654
06 Dec 2014 11:17:41,450 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.stop.time == 1417844861450
06 Dec 2014 11:17:41,450 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.capacity == 100
06 Dec 2014 11:17:41,450 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.current.size == 0
06 Dec 2014 11:17:41,450 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.attempt == 26
06 Dec 2014 11:17:41,451 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.success == 26
06 Dec 2014 11:17:41,451 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.attempt == 52
06 Dec 2014 11:17:41,451 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.success == 23
06 Dec 2014 11:17:41,451 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:83)  - Configuration provider stopping
06 Dec 2014 11:17:41,453 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:422)  - process failed
java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:517)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:317)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:405)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
06 Dec 2014 11:17:41,454 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:160)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:426)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:517)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:317)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:405)
	... 3 more
06 Dec 2014 11:17:46,455 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:437)  - Closing hdfs://hacluster:8020/flumetest/FlumeData
06 Dec 2014 11:17:46,474 INFO  [hdfs-hdfsSink-call-runner-7] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1417844731679.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1417844731679
06 Dec 2014 11:17:46,480 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfsSink stopped
06 Dec 2014 11:17:46,480 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.start.time == 1417844727658
06 Dec 2014 11:17:46,480 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.stop.time == 1417844866480
06 Dec 2014 11:17:46,480 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.complete == 0
06 Dec 2014 11:17:46,480 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.empty == 17
06 Dec 2014 11:17:46,480 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.underflow == 9
06 Dec 2014 11:17:46,481 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.closed.count == 7
06 Dec 2014 11:17:46,481 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.creation.count == 7
06 Dec 2014 11:17:46,481 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.failed.count == 0
06 Dec 2014 11:17:46,481 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.attempt == 26
06 Dec 2014 11:17:46,481 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.sucess == 23
06 Dec 2014 11:17:51,231 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
06 Dec 2014 11:17:51,237 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
06 Dec 2014 11:17:51,244 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:17:51,244 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:17:51,244 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfsSink Agent: agent
06 Dec 2014 11:17:51,244 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:17:51,244 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:17:51,244 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:17:51,262 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
06 Dec 2014 11:17:51,262 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
06 Dec 2014 11:17:51,274 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memoryChannel type memory
06 Dec 2014 11:17:51,279 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memoryChannel
06 Dec 2014 11:17:51,280 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source logstream, type exec
06 Dec 2014 11:17:51,295 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfsSink, type: hdfs
06 Dec 2014 11:17:51,681 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
06 Dec 2014 11:17:52,026 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
06 Dec 2014 11:17:52,035 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memoryChannel connected to [logstream, hdfsSink]
06 Dec 2014 11:17:52,062 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{logstream=EventDrivenSourceRunner: { source:org.apache.flume.source.ExecSource{name:logstream,state:IDLE} }} sinkRunners:{hdfsSink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2366c122 counterGroup:{ name:null counters:{} } }} channels:{memoryChannel=org.apache.flume.channel.MemoryChannel{name: memoryChannel}} }
06 Dec 2014 11:17:52,081 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memoryChannel
06 Dec 2014 11:17:52,161 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memoryChannel, registered successfully.
06 Dec 2014 11:17:52,161 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memoryChannel started
06 Dec 2014 11:17:52,161 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfsSink
06 Dec 2014 11:17:52,162 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source logstream
06 Dec 2014 11:17:52,162 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.ExecSource.start:163)  - Exec source starting with command:tail -f /apache/flume/test
06 Dec 2014 11:17:52,171 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfsSink, registered successfully.
06 Dec 2014 11:17:52,172 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfsSink started
06 Dec 2014 11:17:52,174 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SOURCE, name: logstream, registered successfully.
06 Dec 2014 11:17:52,174 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SOURCE, name: logstream started
06 Dec 2014 11:17:52,177 INFO  [pool-3-thread-1] (org.apache.flume.source.ExecSource$ExecRunnable.run:370)  - Command [tail -f /apache/flume/test] exited with 1
06 Dec 2014 11:18:26,363 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:79)  - Stopping lifecycle supervisor 10
06 Dec 2014 11:18:26,367 INFO  [agent-shutdown-hook] (org.apache.flume.source.ExecSource.stop:186)  - Stopping exec source with command:tail -f /apache/flume/test
06 Dec 2014 11:18:26,367 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SOURCE, name: logstream stopped
06 Dec 2014 11:18:26,367 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SOURCE, name: logstream. source.start.time == 1417844872174
06 Dec 2014 11:18:26,367 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SOURCE, name: logstream. source.stop.time == 1417844906367
06 Dec 2014 11:18:26,368 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append-batch.accepted == 0
06 Dec 2014 11:18:26,368 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append-batch.received == 0
06 Dec 2014 11:18:26,368 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append.accepted == 0
06 Dec 2014 11:18:26,368 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append.received == 0
06 Dec 2014 11:18:26,368 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.events.accepted == 0
06 Dec 2014 11:18:26,368 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.events.received == 0
06 Dec 2014 11:18:26,368 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.open-connection.count == 0
06 Dec 2014 11:18:26,368 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memoryChannel stopped
06 Dec 2014 11:18:26,368 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.start.time == 1417844872161
06 Dec 2014 11:18:26,369 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.stop.time == 1417844906368
06 Dec 2014 11:18:26,369 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.capacity == 100
06 Dec 2014 11:18:26,369 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.current.size == 0
06 Dec 2014 11:18:26,369 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.attempt == 0
06 Dec 2014 11:18:26,369 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.success == 0
06 Dec 2014 11:18:26,369 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.attempt == 6
06 Dec 2014 11:18:26,369 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.success == 0
06 Dec 2014 11:18:26,369 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:83)  - Configuration provider stopping
06 Dec 2014 11:18:26,370 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfsSink stopped
06 Dec 2014 11:18:26,370 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.start.time == 1417844872172
06 Dec 2014 11:18:26,370 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.stop.time == 1417844906370
06 Dec 2014 11:18:26,376 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.complete == 0
06 Dec 2014 11:18:26,376 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.empty == 6
06 Dec 2014 11:18:26,376 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.underflow == 0
06 Dec 2014 11:18:26,376 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.closed.count == 0
06 Dec 2014 11:18:26,376 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.creation.count == 0
06 Dec 2014 11:18:26,376 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.failed.count == 0
06 Dec 2014 11:18:26,377 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.attempt == 0
06 Dec 2014 11:18:26,377 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.sucess == 0
06 Dec 2014 11:30:21,339 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
06 Dec 2014 11:30:21,344 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
06 Dec 2014 11:30:21,352 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:30:21,352 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:30:21,352 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfsSink Agent: agent
06 Dec 2014 11:30:21,352 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:30:21,352 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:30:21,352 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 11:30:21,374 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
06 Dec 2014 11:30:21,374 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
06 Dec 2014 11:30:21,385 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memoryChannel type memory
06 Dec 2014 11:30:21,398 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memoryChannel
06 Dec 2014 11:30:21,399 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source logstream, type exec
06 Dec 2014 11:30:21,413 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfsSink, type: hdfs
06 Dec 2014 11:30:21,849 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
06 Dec 2014 11:30:22,212 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
06 Dec 2014 11:30:22,225 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memoryChannel connected to [logstream, hdfsSink]
06 Dec 2014 11:30:22,252 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{logstream=EventDrivenSourceRunner: { source:org.apache.flume.source.ExecSource{name:logstream,state:IDLE} }} sinkRunners:{hdfsSink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2366c122 counterGroup:{ name:null counters:{} } }} channels:{memoryChannel=org.apache.flume.channel.MemoryChannel{name: memoryChannel}} }
06 Dec 2014 11:30:22,267 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memoryChannel
06 Dec 2014 11:30:22,340 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memoryChannel, registered successfully.
06 Dec 2014 11:30:22,340 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memoryChannel started
06 Dec 2014 11:30:22,340 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfsSink
06 Dec 2014 11:30:22,341 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source logstream
06 Dec 2014 11:30:22,341 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.ExecSource.start:163)  - Exec source starting with command:tail -f /apache/flume/test
06 Dec 2014 11:30:22,344 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfsSink, registered successfully.
06 Dec 2014 11:30:22,344 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfsSink started
06 Dec 2014 11:30:22,351 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SOURCE, name: logstream, registered successfully.
06 Dec 2014 11:30:22,351 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SOURCE, name: logstream started
06 Dec 2014 11:30:26,360 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSSequenceFile.configure:63)  - writeFormat = Text, UseRawLocalFileSystem = false
06 Dec 2014 11:30:26,488 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417845626358.tmp
06 Dec 2014 11:30:58,263 INFO  [hdfs-hdfsSink-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1417845626358.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1417845626358
06 Dec 2014 11:32:46,862 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417845626359.tmp
06 Dec 2014 11:33:17,016 INFO  [hdfs-hdfsSink-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1417845626359.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1417845626359
06 Dec 2014 13:16:46,127 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:79)  - Stopping lifecycle supervisor 10
06 Dec 2014 13:16:46,131 INFO  [agent-shutdown-hook] (org.apache.flume.source.ExecSource.stop:186)  - Stopping exec source with command:tail -f /apache/flume/test
06 Dec 2014 13:16:46,133 INFO  [pool-3-thread-1] (org.apache.flume.source.ExecSource$ExecRunnable.run:370)  - Command [tail -f /apache/flume/test] exited with 143
06 Dec 2014 13:16:46,133 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SOURCE, name: logstream stopped
06 Dec 2014 13:16:46,133 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SOURCE, name: logstream. source.start.time == 1417845622351
06 Dec 2014 13:16:46,133 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SOURCE, name: logstream. source.stop.time == 1417852006133
06 Dec 2014 13:16:46,133 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append-batch.accepted == 0
06 Dec 2014 13:16:46,133 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append-batch.received == 0
06 Dec 2014 13:16:46,134 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append.accepted == 0
06 Dec 2014 13:16:46,134 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append.received == 0
06 Dec 2014 13:16:46,134 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.events.accepted == 9
06 Dec 2014 13:16:46,134 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.events.received == 9
06 Dec 2014 13:16:46,134 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.open-connection.count == 0
06 Dec 2014 13:16:46,134 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memoryChannel stopped
06 Dec 2014 13:16:46,135 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.start.time == 1417845622340
06 Dec 2014 13:16:46,135 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.stop.time == 1417852006134
06 Dec 2014 13:16:46,135 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.capacity == 100
06 Dec 2014 13:16:46,135 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.current.size == 0
06 Dec 2014 13:16:46,135 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.attempt == 9
06 Dec 2014 13:16:46,141 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.success == 9
06 Dec 2014 13:16:46,141 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.attempt == 813
06 Dec 2014 13:16:46,141 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.success == 9
06 Dec 2014 13:16:46,141 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:83)  - Configuration provider stopping
06 Dec 2014 13:16:46,142 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:437)  - Closing hdfs://hacluster:8020/flumetest/FlumeData
06 Dec 2014 13:16:46,142 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.close:296)  - HDFSWriter is already closed: hdfs://hacluster:8020/flumetest/FlumeData.1417845626359.tmp
06 Dec 2014 13:16:46,143 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfsSink stopped
06 Dec 2014 13:16:46,143 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.start.time == 1417845622344
06 Dec 2014 13:16:46,143 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.stop.time == 1417852006143
06 Dec 2014 13:16:46,143 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.complete == 0
06 Dec 2014 13:16:46,143 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.empty == 799
06 Dec 2014 13:16:46,144 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.underflow == 5
06 Dec 2014 13:16:46,144 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.closed.count == 2
06 Dec 2014 13:16:46,144 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.creation.count == 2
06 Dec 2014 13:16:46,144 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.failed.count == 0
06 Dec 2014 13:16:46,144 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.attempt == 9
06 Dec 2014 13:16:46,144 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.sucess == 9
06 Dec 2014 13:18:30,258 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
06 Dec 2014 13:18:30,263 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
06 Dec 2014 13:18:30,272 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 13:18:30,272 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 13:18:30,272 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfsSink Agent: agent
06 Dec 2014 13:18:30,272 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 13:18:30,272 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 13:18:30,272 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
06 Dec 2014 13:18:30,292 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
06 Dec 2014 13:18:30,292 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
06 Dec 2014 13:18:30,306 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memoryChannel type memory
06 Dec 2014 13:18:30,315 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memoryChannel
06 Dec 2014 13:18:30,328 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source logstream, type exec
06 Dec 2014 13:18:30,339 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfsSink, type: hdfs
06 Dec 2014 13:18:30,830 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
06 Dec 2014 13:18:31,220 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
06 Dec 2014 13:18:31,233 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memoryChannel connected to [logstream, hdfsSink]
06 Dec 2014 13:18:31,272 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{logstream=EventDrivenSourceRunner: { source:org.apache.flume.source.ExecSource{name:logstream,state:IDLE} }} sinkRunners:{hdfsSink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2366c122 counterGroup:{ name:null counters:{} } }} channels:{memoryChannel=org.apache.flume.channel.MemoryChannel{name: memoryChannel}} }
06 Dec 2014 13:18:31,286 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memoryChannel
06 Dec 2014 13:18:31,375 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memoryChannel, registered successfully.
06 Dec 2014 13:18:31,375 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memoryChannel started
06 Dec 2014 13:18:31,375 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfsSink
06 Dec 2014 13:18:31,377 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source logstream
06 Dec 2014 13:18:31,377 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.ExecSource.start:163)  - Exec source starting with command:tail -f /apache/flume/test
06 Dec 2014 13:18:31,380 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfsSink, registered successfully.
06 Dec 2014 13:18:31,381 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfsSink started
06 Dec 2014 13:18:31,382 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SOURCE, name: logstream, registered successfully.
06 Dec 2014 13:18:31,382 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SOURCE, name: logstream started
06 Dec 2014 13:18:35,403 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSSequenceFile.configure:63)  - writeFormat = Text, UseRawLocalFileSystem = false
06 Dec 2014 13:18:35,555 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417852115399.tmp
06 Dec 2014 13:19:07,557 INFO  [hdfs-hdfsSink-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1417852115399.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1417852115399
06 Dec 2014 13:19:26,817 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417852115400.tmp
06 Dec 2014 13:19:56,953 INFO  [hdfs-hdfsSink-call-runner-5] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1417852115400.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1417852115400
06 Dec 2014 13:21:16,496 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417852115401.tmp
06 Dec 2014 13:21:25,467 INFO  [hdfs-hdfsSink-call-runner-2] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1417852115401.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1417852115401
06 Dec 2014 13:21:25,506 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1417852115402.tmp
06 Dec 2014 13:21:55,578 INFO  [hdfs-hdfsSink-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1417852115402.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1417852115402
10 Dec 2014 04:12:30,960 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
10 Dec 2014 04:12:30,968 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
10 Dec 2014 04:12:30,974 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
10 Dec 2014 04:12:30,975 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
10 Dec 2014 04:12:30,975 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfsSink Agent: agent
10 Dec 2014 04:12:30,975 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
10 Dec 2014 04:12:30,975 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
10 Dec 2014 04:12:30,975 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
10 Dec 2014 04:12:30,993 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
10 Dec 2014 04:12:30,993 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
10 Dec 2014 04:12:31,005 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memoryChannel type memory
10 Dec 2014 04:12:31,014 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memoryChannel
10 Dec 2014 04:12:31,015 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source logstream, type exec
10 Dec 2014 04:12:31,027 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfsSink, type: hdfs
10 Dec 2014 04:12:31,401 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
10 Dec 2014 04:12:31,728 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
10 Dec 2014 04:12:31,737 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memoryChannel connected to [logstream, hdfsSink]
10 Dec 2014 04:12:31,767 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{logstream=EventDrivenSourceRunner: { source:org.apache.flume.source.ExecSource{name:logstream,state:IDLE} }} sinkRunners:{hdfsSink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2366c122 counterGroup:{ name:null counters:{} } }} channels:{memoryChannel=org.apache.flume.channel.MemoryChannel{name: memoryChannel}} }
10 Dec 2014 04:12:31,780 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memoryChannel
10 Dec 2014 04:12:31,859 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memoryChannel, registered successfully.
10 Dec 2014 04:12:31,859 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memoryChannel started
10 Dec 2014 04:12:31,860 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfsSink
10 Dec 2014 04:12:31,860 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source logstream
10 Dec 2014 04:12:31,861 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.ExecSource.start:163)  - Exec source starting with command:tail -f /apache/flume/test
10 Dec 2014 04:12:31,868 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfsSink, registered successfully.
10 Dec 2014 04:12:31,868 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfsSink started
10 Dec 2014 04:12:31,870 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SOURCE, name: logstream, registered successfully.
10 Dec 2014 04:12:31,870 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SOURCE, name: logstream started
10 Dec 2014 04:12:35,881 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSSequenceFile.configure:63)  - writeFormat = Text, UseRawLocalFileSystem = false
10 Dec 2014 04:12:35,997 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955879.tmp
10 Dec 2014 04:12:46,007 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955879.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:12:46,009 WARN  [hdfs-hdfsSink-call-runner-0] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:12:48,049 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955880.tmp
10 Dec 2014 04:12:58,049 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955880.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:12:58,053 WARN  [hdfs-hdfsSink-call-runner-1] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:13:01,158 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955881.tmp
10 Dec 2014 04:13:11,161 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955881.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:13:11,162 WARN  [hdfs-hdfsSink-call-runner-2] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:13:15,185 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955882.tmp
10 Dec 2014 04:13:25,185 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955882.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:13:25,186 WARN  [hdfs-hdfsSink-call-runner-3] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:13:30,315 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955883.tmp
10 Dec 2014 04:13:40,321 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955883.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:13:40,322 WARN  [hdfs-hdfsSink-call-runner-4] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:13:45,344 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955884.tmp
10 Dec 2014 04:13:55,344 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955884.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:13:55,345 WARN  [hdfs-hdfsSink-call-runner-5] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:14:00,369 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955885.tmp
10 Dec 2014 04:14:10,370 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955885.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:14:10,370 WARN  [hdfs-hdfsSink-call-runner-6] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:14:15,398 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955886.tmp
10 Dec 2014 04:14:25,399 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955886.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:14:25,400 WARN  [hdfs-hdfsSink-call-runner-7] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:14:29,107 WARN  [hdfs-hdfsSink-call-runner-1] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create over nn2.hadoop.com/192.168.1.9:8020. Not retrying because retries (11) exceeded maximum allowed (10)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955880.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1211)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2235)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2190)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:520)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955880.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1207)
	... 13 more

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:258)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:14:30,429 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955887.tmp
10 Dec 2014 04:14:30,732 WARN  [hdfs-hdfsSink-call-runner-3] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create over nn2.hadoop.com/192.168.1.9:8020. Not retrying because retries (11) exceeded maximum allowed (10)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955882.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1211)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2235)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2190)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:520)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955882.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1207)
	... 13 more

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:258)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:14:36,707 WARN  [hdfs-hdfsSink-call-runner-0] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create over nn2.hadoop.com/192.168.1.9:8020. Not retrying because retries (12) exceeded maximum allowed (10)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955879.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1211)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2235)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2190)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:520)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955879.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1207)
	... 13 more

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:258)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:14:38,266 WARN  [hdfs-hdfsSink-call-runner-2] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create over nn2.hadoop.com/192.168.1.9:8020. Not retrying because retries (11) exceeded maximum allowed (10)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955881.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1211)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2235)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2190)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:520)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955881.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1207)
	... 13 more

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:258)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:14:40,435 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955887.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:14:40,435 WARN  [hdfs-hdfsSink-call-runner-8] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:14:45,455 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955888.tmp
10 Dec 2014 04:14:55,456 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955888.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:14:55,456 WARN  [hdfs-hdfsSink-call-runner-9] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:15:00,476 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955889.tmp
10 Dec 2014 04:15:10,481 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955889.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:15:10,481 WARN  [hdfs-hdfsSink-call-runner-1] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:15:15,502 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955890.tmp
10 Dec 2014 04:15:17,677 WARN  [hdfs-hdfsSink-call-runner-4] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create over nn2.hadoop.com/192.168.1.9:8020. Not retrying because retries (11) exceeded maximum allowed (10)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955883.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1211)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2235)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2190)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:520)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955883.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1207)
	... 13 more

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:258)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:15:25,509 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955890.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:15:25,510 WARN  [hdfs-hdfsSink-call-runner-3] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:15:27,906 WARN  [hdfs-hdfsSink-call-runner-5] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create over nn2.hadoop.com/192.168.1.9:8020. Not retrying because retries (11) exceeded maximum allowed (10)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955884.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1211)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2235)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2190)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:520)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955884.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1207)
	... 13 more

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:258)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:15:30,528 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955891.tmp
10 Dec 2014 04:15:38,754 WARN  [hdfs-hdfsSink-call-runner-7] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create over nn2.hadoop.com/192.168.1.9:8020. Not retrying because retries (11) exceeded maximum allowed (10)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955886.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1211)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2235)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2190)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:520)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955886.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1207)
	... 13 more

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:258)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:15:40,534 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955891.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:15:40,535 WARN  [hdfs-hdfsSink-call-runner-0] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:15:45,555 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955892.tmp
10 Dec 2014 04:15:55,558 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955892.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:15:55,559 WARN  [hdfs-hdfsSink-call-runner-2] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:15:55,644 WARN  [hdfs-hdfsSink-call-runner-6] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create over nn2.hadoop.com/192.168.1.9:8020. Not retrying because retries (11) exceeded maximum allowed (10)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955885.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1211)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2235)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2190)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:520)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955885.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1207)
	... 13 more

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:258)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:16:00,577 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955893.tmp
10 Dec 2014 04:16:10,582 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955893.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:16:10,582 WARN  [hdfs-hdfsSink-call-runner-4] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:16:15,613 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955894.tmp
10 Dec 2014 04:16:16,678 WARN  [hdfs-hdfsSink-call-runner-8] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create over nn2.hadoop.com/192.168.1.9:8020. Not retrying because retries (11) exceeded maximum allowed (10)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955887.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1211)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2235)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2190)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:520)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955887.tmp. Name node is in safe mode.
The reported blocks 0 needs additional 167 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1207)
	... 13 more

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:258)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:16:25,618 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955894.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:16:25,619 WARN  [hdfs-hdfsSink-call-runner-5] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:16:30,652 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955895.tmp
10 Dec 2014 04:16:38,639 WARN  [hdfs-hdfsSink-call-runner-9] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create over nn2.hadoop.com/192.168.1.9:8020. Not retrying because retries (11) exceeded maximum allowed (10)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955888.tmp. Name node is in safe mode.
The reported blocks 165 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 1 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1211)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2235)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2190)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:520)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955888.tmp. Name node is in safe mode.
The reported blocks 165 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 1 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1207)
	... 13 more

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:258)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:16:40,657 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955895.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:16:40,657 WARN  [hdfs-hdfsSink-call-runner-7] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:16:44,680 WARN  [hdfs-hdfsSink-call-runner-1] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create over nn2.hadoop.com/192.168.1.9:8020. Not retrying because retries (11) exceeded maximum allowed (10)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955889.tmp. Name node is in safe mode.
The reported blocks 165 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 1 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1211)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2235)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2190)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:520)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955889.tmp. Name node is in safe mode.
The reported blocks 165 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 1 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1207)
	... 13 more

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:258)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:16:45,677 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955896.tmp
10 Dec 2014 04:16:55,681 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955896.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:16:55,682 WARN  [hdfs-hdfsSink-call-runner-6] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:17:00,066 WARN  [hdfs-hdfsSink-call-runner-3] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create over nn2.hadoop.com/192.168.1.9:8020. Not retrying because retries (11) exceeded maximum allowed (10)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955890.tmp. Name node is in safe mode.
The reported blocks 165 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 1 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1211)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2235)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2190)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:520)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955890.tmp. Name node is in safe mode.
The reported blocks 165 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 167.
The number of live datanodes 1 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1207)
	... 13 more

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:258)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:17:00,711 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955897.tmp
10 Dec 2014 04:17:10,717 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955897.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:17:10,717 WARN  [hdfs-hdfsSink-call-runner-8] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:17:15,734 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955898.tmp
10 Dec 2014 04:17:25,740 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955898.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:17:25,740 WARN  [hdfs-hdfsSink-call-runner-9] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:17:30,762 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955899.tmp
10 Dec 2014 04:17:32,203 WARN  [hdfs-hdfsSink-call-runner-2] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create over nn2.hadoop.com/192.168.1.9:8020. Not retrying because retries (11) exceeded maximum allowed (10)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955892.tmp. Name node is in safe mode.
The reported blocks 166 has reached the threshold 0.9990 of total blocks 167. The number of live datanodes 2 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 5 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1211)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2235)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2190)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:520)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955892.tmp. Name node is in safe mode.
The reported blocks 166 has reached the threshold 0.9990 of total blocks 167. The number of live datanodes 2 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 5 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1207)
	... 13 more

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:258)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:17:37,892 WARN  [hdfs-hdfsSink-call-runner-0] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create over nn2.hadoop.com/192.168.1.9:8020. Not retrying because retries (11) exceeded maximum allowed (10)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955891.tmp. Name node is in safe mode.
The reported blocks 166 has reached the threshold 0.9990 of total blocks 167. The number of live datanodes 2 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1211)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2235)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2190)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:520)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create file/flumetest/FlumeData.1418164955891.tmp. Name node is in safe mode.
The reported blocks 166 has reached the threshold 0.9990 of total blocks 167. The number of live datanodes 2 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1207)
	... 13 more

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:258)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:17:40,767 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:418)  - HDFS IO error
java.io.IOException: Callable timed out after 10000 ms on file: hdfs://hacluster:8020/flumetest/FlumeData.1418164955899.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:550)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:383)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:392)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:201)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:543)
	... 6 more
10 Dec 2014 04:17:40,767 WARN  [hdfs-hdfsSink-call-runner-1] (org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts:45)  - interrupted while sleeping
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.util.ThreadUtil.sleepAtLeastIgnoreInterrupts(ThreadUtil.java:43)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:154)
	at com.sun.proxy.$Proxy17.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1600)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1465)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1390)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:390)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:334)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:90)
	at org.apache.flume.sink.hdfs.HDFSSequenceFile.open(HDFSSequenceFile.java:69)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:227)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:220)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:536)
	at org.apache.flume.sink.hdfs.BucketWriter.runPrivileged(BucketWriter.java:160)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:56)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:533)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:17:45,930 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955900.tmp
10 Dec 2014 04:17:47,013 INFO  [hdfs-hdfsSink-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1418164955900.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1418164955900
10 Dec 2014 04:17:47,145 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955901.tmp
10 Dec 2014 04:18:17,352 INFO  [hdfs-hdfsSink-call-runner-5] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1418164955901.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1418164955901
10 Dec 2014 04:19:56,478 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955902.tmp
10 Dec 2014 04:20:17,509 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.append:400)  - Block Under-replication detected. Rotating file.
10 Dec 2014 04:20:17,926 INFO  [hdfs-hdfsSink-call-runner-7] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1418164955902.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1418164955902
10 Dec 2014 04:20:17,949 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418164955903.tmp
10 Dec 2014 04:20:47,996 INFO  [hdfs-hdfsSink-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1418164955903.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1418164955903
10 Dec 2014 04:22:31,880 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
10 Dec 2014 04:22:31,880 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
10 Dec 2014 04:22:31,880 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
10 Dec 2014 04:22:31,880 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfsSink Agent: agent
10 Dec 2014 04:22:31,881 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
10 Dec 2014 04:22:31,881 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
10 Dec 2014 04:22:31,881 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
10 Dec 2014 04:22:31,884 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
10 Dec 2014 04:22:31,885 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
10 Dec 2014 04:22:31,885 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memoryChannel
10 Dec 2014 04:22:31,885 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source netsource, type exec
10 Dec 2014 04:22:31,885 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfsSink, type: hdfs
10 Dec 2014 04:22:31,885 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
10 Dec 2014 04:22:31,886 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memoryChannel connected to [netsource, hdfsSink]
10 Dec 2014 04:22:31,886 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.stopAllComponents:101)  - Shutting down configuration: { sourceRunners:{logstream=EventDrivenSourceRunner: { source:org.apache.flume.source.ExecSource{name:logstream,state:START} }} sinkRunners:{hdfsSink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2366c122 counterGroup:{ name:null counters:{runner.backoffs.consecutive=18, runner.backoffs=60} } }} channels:{memoryChannel=org.apache.flume.channel.MemoryChannel{name: memoryChannel}} }
10 Dec 2014 04:22:31,886 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.stopAllComponents:105)  - Stopping Source logstream
10 Dec 2014 04:22:31,886 INFO  [conf-file-poller-0] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:171)  - Stopping component: EventDrivenSourceRunner: { source:org.apache.flume.source.ExecSource{name:logstream,state:START} }
10 Dec 2014 04:22:31,887 INFO  [conf-file-poller-0] (org.apache.flume.source.ExecSource.stop:186)  - Stopping exec source with command:tail -f /apache/flume/test
10 Dec 2014 04:22:31,887 INFO  [pool-3-thread-1] (org.apache.flume.source.ExecSource$ExecRunnable.run:370)  - Command [tail -f /apache/flume/test] exited with 143
10 Dec 2014 04:22:31,888 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SOURCE, name: logstream stopped
10 Dec 2014 04:22:31,889 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SOURCE, name: logstream. source.start.time == 1418164951870
10 Dec 2014 04:22:31,889 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SOURCE, name: logstream. source.stop.time == 1418165551888
10 Dec 2014 04:22:31,889 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append-batch.accepted == 0
10 Dec 2014 04:22:31,889 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append-batch.received == 0
10 Dec 2014 04:22:31,889 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append.accepted == 0
10 Dec 2014 04:22:31,889 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append.received == 0
10 Dec 2014 04:22:31,889 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.events.accepted == 15
10 Dec 2014 04:22:31,889 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.events.received == 15
10 Dec 2014 04:22:31,890 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.open-connection.count == 0
10 Dec 2014 04:22:31,890 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.stopAllComponents:115)  - Stopping Sink hdfsSink
10 Dec 2014 04:22:31,890 INFO  [conf-file-poller-0] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:171)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2366c122 counterGroup:{ name:null counters:{runner.backoffs.consecutive=18, runner.backoffs=60} } }
10 Dec 2014 04:22:31,891 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:437)  - Closing hdfs://hacluster:8020/flumetest/FlumeData
10 Dec 2014 04:22:31,891 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.BucketWriter.close:296)  - HDFSWriter is already closed: hdfs://hacluster:8020/flumetest/FlumeData.1418164955903.tmp
10 Dec 2014 04:22:31,892 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfsSink stopped
10 Dec 2014 04:22:31,892 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.start.time == 1418164951868
10 Dec 2014 04:22:31,892 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.stop.time == 1418165551892
10 Dec 2014 04:22:31,892 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.complete == 0
10 Dec 2014 04:22:31,892 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.empty == 39
10 Dec 2014 04:22:31,893 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.underflow == 3
10 Dec 2014 04:22:31,893 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.closed.count == 4
10 Dec 2014 04:22:31,893 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.creation.count == 4
10 Dec 2014 04:22:31,893 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.failed.count == 42
10 Dec 2014 04:22:31,893 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.attempt == 15
10 Dec 2014 04:22:31,893 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.sucess == 15
10 Dec 2014 04:22:31,893 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.stopAllComponents:125)  - Stopping Channel memoryChannel
10 Dec 2014 04:22:31,893 INFO  [conf-file-poller-0] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:171)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: memoryChannel}
10 Dec 2014 04:22:31,894 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memoryChannel stopped
10 Dec 2014 04:22:31,894 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.start.time == 1418164951859
10 Dec 2014 04:22:31,894 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.stop.time == 1418165551894
10 Dec 2014 04:22:31,894 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.capacity == 100
10 Dec 2014 04:22:31,894 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.current.size == 0
10 Dec 2014 04:22:31,894 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.attempt == 15
10 Dec 2014 04:22:31,894 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.success == 15
10 Dec 2014 04:22:31,894 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.attempt == 78
10 Dec 2014 04:22:31,894 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.success == 15
10 Dec 2014 04:22:31,895 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{netsource=EventDrivenSourceRunner: { source:org.apache.flume.source.ExecSource{name:netsource,state:IDLE} }} sinkRunners:{hdfsSink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@6412d9af counterGroup:{ name:null counters:{} } }} channels:{memoryChannel=org.apache.flume.channel.MemoryChannel{name: memoryChannel}} }
10 Dec 2014 04:22:31,895 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memoryChannel
10 Dec 2014 04:22:31,895 INFO  [lifecycleSupervisor-1-8] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memoryChannel started
10 Dec 2014 04:22:31,895 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfsSink
10 Dec 2014 04:22:31,896 ERROR [lifecycleSupervisor-1-4] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:113)  - Failed to register monitored counter group for type: SINK, name: hdfsSink
javax.management.InstanceAlreadyExistsException: org.apache.flume.sink:type=hdfsSink
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:108)
	at org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:88)
	at org.apache.flume.sink.hdfs.HDFSEventSink.start(HDFSEventSink.java:484)
	at org.apache.flume.sink.DefaultSinkProcessor.start(DefaultSinkProcessor.java:46)
	at org.apache.flume.SinkRunner.start(SinkRunner.java:79)
	at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:251)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:22:31,896 INFO  [lifecycleSupervisor-1-4] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfsSink started
10 Dec 2014 04:22:31,897 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source netsource
10 Dec 2014 04:22:31,900 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.source.ExecSource.start:163)  - Exec source starting with command:tail -f /apache/flume/test
10 Dec 2014 04:22:31,900 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SOURCE, name: netsource, registered successfully.
10 Dec 2014 04:22:31,900 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SOURCE, name: netsource started
10 Dec 2014 04:22:35,902 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSSequenceFile.configure:63)  - writeFormat = Text, UseRawLocalFileSystem = false
10 Dec 2014 04:22:35,921 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1418165555903.tmp
10 Dec 2014 04:23:06,387 INFO  [hdfs-hdfsSink-call-runner-3] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1418165555903.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1418165555903
10 Dec 2014 04:34:55,332 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:79)  - Stopping lifecycle supervisor 10
10 Dec 2014 04:34:55,336 INFO  [agent-shutdown-hook] (org.apache.flume.source.ExecSource.stop:186)  - Stopping exec source with command:tail -f /apache/flume/test
10 Dec 2014 04:34:55,337 INFO  [pool-12-thread-1] (org.apache.flume.source.ExecSource$ExecRunnable.run:370)  - Command [tail -f /apache/flume/test] exited with 143
10 Dec 2014 04:34:55,337 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SOURCE, name: netsource stopped
10 Dec 2014 04:34:55,337 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SOURCE, name: netsource. source.start.time == 1418165551900
10 Dec 2014 04:34:55,337 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SOURCE, name: netsource. source.stop.time == 1418166295337
10 Dec 2014 04:34:55,337 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: netsource. src.append-batch.accepted == 0
10 Dec 2014 04:34:55,337 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: netsource. src.append-batch.received == 0
10 Dec 2014 04:34:55,338 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: netsource. src.append.accepted == 0
10 Dec 2014 04:34:55,338 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: netsource. src.append.received == 0
10 Dec 2014 04:34:55,338 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: netsource. src.events.accepted == 10
10 Dec 2014 04:34:55,338 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: netsource. src.events.received == 10
10 Dec 2014 04:34:55,338 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: netsource. src.open-connection.count == 0
10 Dec 2014 04:34:55,338 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memoryChannel stopped
10 Dec 2014 04:34:55,338 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.start.time == 1418165551895
10 Dec 2014 04:34:55,338 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.stop.time == 1418166295338
10 Dec 2014 04:34:55,338 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.capacity == 100
10 Dec 2014 04:34:55,338 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.current.size == 0
10 Dec 2014 04:34:55,338 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.attempt == 10
10 Dec 2014 04:34:55,338 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.success == 10
10 Dec 2014 04:34:55,339 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.attempt == 106
10 Dec 2014 04:34:55,339 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.success == 10
10 Dec 2014 04:34:55,339 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:83)  - Configuration provider stopping
10 Dec 2014 04:34:55,339 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:437)  - Closing hdfs://hacluster:8020/flumetest/FlumeData
10 Dec 2014 04:34:55,339 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.close:296)  - HDFSWriter is already closed: hdfs://hacluster:8020/flumetest/FlumeData.1418165555903.tmp
10 Dec 2014 04:34:55,340 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfsSink stopped
10 Dec 2014 04:34:55,340 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.start.time == 1418165551896
10 Dec 2014 04:34:55,340 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.stop.time == 1418166295340
10 Dec 2014 04:34:55,341 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.complete == 0
10 Dec 2014 04:34:55,341 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.empty == 95
10 Dec 2014 04:34:55,341 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.underflow == 1
10 Dec 2014 04:34:55,342 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.closed.count == 1
10 Dec 2014 04:34:55,342 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.creation.count == 1
10 Dec 2014 04:34:55,342 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.failed.count == 0
10 Dec 2014 04:34:55,342 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.attempt == 10
10 Dec 2014 04:34:55,342 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.sucess == 10
10 Dec 2014 04:35:06,696 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
10 Dec 2014 04:35:06,701 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
10 Dec 2014 04:35:06,708 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:35:06,708 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfssink Agent: agent
10 Dec 2014 04:35:06,708 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:35:06,708 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:35:06,708 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:35:06,708 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:35:06,709 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:35:06,709 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:35:06,725 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
10 Dec 2014 04:35:06,726 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
10 Dec 2014 04:35:06,738 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memorychannel type memory
10 Dec 2014 04:35:06,747 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memorychannel
10 Dec 2014 04:35:06,748 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source netsource, type netcat
10 Dec 2014 04:35:06,768 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfssink, type: hdfs
10 Dec 2014 04:35:07,245 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
10 Dec 2014 04:35:07,574 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
10 Dec 2014 04:35:07,576 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memorychannel connected to [netsource, hdfssink]
10 Dec 2014 04:35:07,592 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{netsource=EventDrivenSourceRunner: { source:org.apache.flume.source.NetcatSource{name:netsource,state:IDLE} }} sinkRunners:{hdfssink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@362f7b99 counterGroup:{ name:null counters:{} } }} channels:{memorychannel=org.apache.flume.channel.MemoryChannel{name: memorychannel}} }
10 Dec 2014 04:35:07,607 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memorychannel
10 Dec 2014 04:35:07,706 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memorychannel, registered successfully.
10 Dec 2014 04:35:07,709 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memorychannel started
10 Dec 2014 04:35:07,710 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfssink
10 Dec 2014 04:35:07,711 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source netsource
10 Dec 2014 04:35:07,711 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.NetcatSource.start:150)  - Source starting
10 Dec 2014 04:35:07,715 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfssink, registered successfully.
10 Dec 2014 04:35:07,716 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfssink started
10 Dec 2014 04:35:07,744 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.NetcatSource.start:164)  - Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:3000]
10 Dec 2014 04:36:19,896 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:79)  - Stopping lifecycle supervisor 10
10 Dec 2014 04:36:19,900 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfssink stopped
10 Dec 2014 04:36:19,900 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfssink. sink.start.time == 1418166307716
10 Dec 2014 04:36:19,901 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfssink. sink.stop.time == 1418166379900
10 Dec 2014 04:36:19,901 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.complete == 0
10 Dec 2014 04:36:19,901 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.empty == 11
10 Dec 2014 04:36:19,902 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.underflow == 0
10 Dec 2014 04:36:19,902 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.closed.count == 0
10 Dec 2014 04:36:19,902 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.creation.count == 0
10 Dec 2014 04:36:19,902 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.failed.count == 0
10 Dec 2014 04:36:19,902 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.event.drain.attempt == 0
10 Dec 2014 04:36:19,902 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.event.drain.sucess == 0
10 Dec 2014 04:36:19,903 INFO  [agent-shutdown-hook] (org.apache.flume.source.NetcatSource.stop:190)  - Source stopping
10 Dec 2014 04:36:19,904 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:83)  - Configuration provider stopping
10 Dec 2014 04:36:19,904 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memorychannel stopped
10 Dec 2014 04:36:19,904 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.start.time == 1418166307709
10 Dec 2014 04:36:19,904 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.stop.time == 1418166379904
10 Dec 2014 04:36:19,904 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.capacity == 1000
10 Dec 2014 04:36:19,904 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.current.size == 0
10 Dec 2014 04:36:19,905 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.put.attempt == 0
10 Dec 2014 04:36:19,905 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.put.success == 0
10 Dec 2014 04:36:19,905 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.take.attempt == 11
10 Dec 2014 04:36:19,905 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.take.success == 0
10 Dec 2014 04:36:24,877 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
10 Dec 2014 04:36:24,883 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
10 Dec 2014 04:36:24,889 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:36:24,889 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfssink Agent: agent
10 Dec 2014 04:36:24,889 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:36:24,889 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:36:24,889 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:36:24,889 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:36:24,890 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:36:24,890 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:36:24,907 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
10 Dec 2014 04:36:24,912 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
10 Dec 2014 04:36:24,926 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memorychannel type memory
10 Dec 2014 04:36:24,931 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memorychannel
10 Dec 2014 04:36:24,932 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source netsource, type netcat
10 Dec 2014 04:36:24,952 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfssink, type: hdfs
10 Dec 2014 04:36:25,406 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
10 Dec 2014 04:36:25,755 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
10 Dec 2014 04:36:25,760 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memorychannel connected to [netsource, hdfssink]
10 Dec 2014 04:36:25,790 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{netsource=EventDrivenSourceRunner: { source:org.apache.flume.source.NetcatSource{name:netsource,state:IDLE} }} sinkRunners:{hdfssink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@362f7b99 counterGroup:{ name:null counters:{} } }} channels:{memorychannel=org.apache.flume.channel.MemoryChannel{name: memorychannel}} }
10 Dec 2014 04:36:25,802 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memorychannel
10 Dec 2014 04:36:25,901 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memorychannel, registered successfully.
10 Dec 2014 04:36:25,902 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memorychannel started
10 Dec 2014 04:36:25,902 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfssink
10 Dec 2014 04:36:25,903 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source netsource
10 Dec 2014 04:36:25,904 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.NetcatSource.start:150)  - Source starting
10 Dec 2014 04:36:25,909 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfssink, registered successfully.
10 Dec 2014 04:36:25,910 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfssink started
10 Dec 2014 04:36:25,928 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.NetcatSource.start:164)  - Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:3000]
10 Dec 2014 04:36:49,044 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:56)  - Serializer = TEXT, UseRawLocalFileSystem = false
10 Dec 2014 04:36:49,181 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flume_network/log.1418166409045.tmp
10 Dec 2014 04:36:58,064 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.append:400)  - Block Under-replication detected. Rotating file.
10 Dec 2014 04:36:58,090 INFO  [hdfs-hdfssink-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flume_network/log.1418166409045.tmp to hdfs://hacluster:8020/flume_network/log.1418166409045
10 Dec 2014 04:36:58,145 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flume_network/log.1418166409046.tmp
10 Dec 2014 04:38:36,798 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.append:400)  - Block Under-replication detected. Rotating file.
10 Dec 2014 04:38:36,849 INFO  [hdfs-hdfssink-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flume_network/log.1418166409046.tmp to hdfs://hacluster:8020/flume_network/log.1418166409046
10 Dec 2014 04:38:36,884 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flume_network/log.1418166409047.tmp
10 Dec 2014 04:38:43,228 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.append:400)  - Block Under-replication detected. Rotating file.
10 Dec 2014 04:38:43,244 INFO  [hdfs-hdfssink-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flume_network/log.1418166409047.tmp to hdfs://hacluster:8020/flume_network/log.1418166409047
10 Dec 2014 04:38:43,278 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flume_network/log.1418166409048.tmp
10 Dec 2014 04:43:55,916 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
10 Dec 2014 04:43:55,917 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:43:55,917 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfssink Agent: agent
10 Dec 2014 04:43:55,917 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:43:55,917 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:43:55,917 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:43:55,917 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:43:55,917 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:43:55,918 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:43:55,923 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
10 Dec 2014 04:43:55,923 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
10 Dec 2014 04:43:55,923 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memorychannel
10 Dec 2014 04:43:55,923 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source netsource, type netcat
10 Dec 2014 04:43:55,923 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfssink, type: hdfs
10 Dec 2014 04:43:55,923 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
10 Dec 2014 04:43:55,926 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memorychannel connected to [netsource, hdfssink]
10 Dec 2014 04:43:55,927 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.stopAllComponents:101)  - Shutting down configuration: { sourceRunners:{netsource=EventDrivenSourceRunner: { source:org.apache.flume.source.NetcatSource{name:netsource,state:START} }} sinkRunners:{hdfssink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@362f7b99 counterGroup:{ name:null counters:{runner.backoffs.consecutive=40, runner.backoffs=58} } }} channels:{memorychannel=org.apache.flume.channel.MemoryChannel{name: memorychannel}} }
10 Dec 2014 04:43:55,927 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.stopAllComponents:105)  - Stopping Source netsource
10 Dec 2014 04:43:55,927 INFO  [conf-file-poller-0] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:171)  - Stopping component: EventDrivenSourceRunner: { source:org.apache.flume.source.NetcatSource{name:netsource,state:START} }
10 Dec 2014 04:43:55,927 INFO  [conf-file-poller-0] (org.apache.flume.source.NetcatSource.stop:190)  - Source stopping
10 Dec 2014 04:43:55,931 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.stopAllComponents:115)  - Stopping Sink hdfssink
10 Dec 2014 04:43:55,932 INFO  [conf-file-poller-0] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:171)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@362f7b99 counterGroup:{ name:null counters:{runner.backoffs.consecutive=40, runner.backoffs=58} } }
10 Dec 2014 04:43:55,932 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:437)  - Closing hdfs://hacluster:8020/flume_network/log
10 Dec 2014 04:43:56,360 INFO  [hdfs-hdfssink-call-runner-1] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flume_network/log.1418166409048.tmp to hdfs://hacluster:8020/flume_network/log.1418166409048
10 Dec 2014 04:43:56,368 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfssink stopped
10 Dec 2014 04:43:56,368 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfssink. sink.start.time == 1418166385910
10 Dec 2014 04:43:56,368 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfssink. sink.stop.time == 1418166836368
10 Dec 2014 04:43:56,368 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.complete == 0
10 Dec 2014 04:43:56,368 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.empty == 58
10 Dec 2014 04:43:56,369 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.underflow == 4
10 Dec 2014 04:43:56,369 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.closed.count == 4
10 Dec 2014 04:43:56,369 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.creation.count == 4
10 Dec 2014 04:43:56,369 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.failed.count == 0
10 Dec 2014 04:43:56,369 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.event.drain.attempt == 6
10 Dec 2014 04:43:56,369 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.event.drain.sucess == 6
10 Dec 2014 04:43:56,369 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.stopAllComponents:125)  - Stopping Channel memorychannel
10 Dec 2014 04:43:56,370 INFO  [conf-file-poller-0] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:171)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: memorychannel}
10 Dec 2014 04:43:56,370 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memorychannel stopped
10 Dec 2014 04:43:56,370 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.start.time == 1418166385902
10 Dec 2014 04:43:56,370 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.stop.time == 1418166836370
10 Dec 2014 04:43:56,370 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.capacity == 1000
10 Dec 2014 04:43:56,370 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.current.size == 0
10 Dec 2014 04:43:56,370 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.put.attempt == 6
10 Dec 2014 04:43:56,370 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.put.success == 6
10 Dec 2014 04:43:56,371 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.take.attempt == 68
10 Dec 2014 04:43:56,371 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.take.success == 6
10 Dec 2014 04:43:56,371 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{netsource=EventDrivenSourceRunner: { source:org.apache.flume.source.NetcatSource{name:netsource,state:IDLE} }} sinkRunners:{hdfssink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@22d7472e counterGroup:{ name:null counters:{} } }} channels:{memorychannel=org.apache.flume.channel.MemoryChannel{name: memorychannel}} }
10 Dec 2014 04:43:56,371 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memorychannel
10 Dec 2014 04:43:56,371 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:230)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@362f7b99 counterGroup:{ name:null counters:{runner.backoffs.consecutive=40, runner.backoffs=58, runner.interruptions=1} } }
10 Dec 2014 04:43:56,371 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memorychannel started
10 Dec 2014 04:43:56,372 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfssink
10 Dec 2014 04:43:56,373 ERROR [lifecycleSupervisor-1-8] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:113)  - Failed to register monitored counter group for type: SINK, name: hdfssink
javax.management.InstanceAlreadyExistsException: org.apache.flume.sink:type=hdfssink
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:108)
	at org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:88)
	at org.apache.flume.sink.hdfs.HDFSEventSink.start(HDFSEventSink.java:484)
	at org.apache.flume.sink.DefaultSinkProcessor.start(DefaultSinkProcessor.java:46)
	at org.apache.flume.SinkRunner.start(SinkRunner.java:79)
	at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:251)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:43:56,375 INFO  [lifecycleSupervisor-1-8] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfssink started
10 Dec 2014 04:43:56,376 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source netsource
10 Dec 2014 04:43:56,378 INFO  [lifecycleSupervisor-1-9] (org.apache.flume.source.NetcatSource.start:150)  - Source starting
10 Dec 2014 04:43:56,379 INFO  [lifecycleSupervisor-1-9] (org.apache.flume.source.NetcatSource.start:164)  - Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:3000]
10 Dec 2014 04:46:26,381 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
10 Dec 2014 04:46:26,382 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:46:26,382 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfssink Agent: agent
10 Dec 2014 04:46:26,382 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:46:26,382 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:46:26,382 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:46:26,382 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:46:26,382 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:46:26,383 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:46:26,388 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
10 Dec 2014 04:46:26,388 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
10 Dec 2014 04:46:26,388 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memorychannel
10 Dec 2014 04:46:26,388 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source netsource, type netcat
10 Dec 2014 04:46:26,391 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfssink, type: hdfs
10 Dec 2014 04:46:26,391 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
10 Dec 2014 04:46:26,391 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memorychannel connected to [netsource, hdfssink]
10 Dec 2014 04:46:26,392 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.stopAllComponents:101)  - Shutting down configuration: { sourceRunners:{netsource=EventDrivenSourceRunner: { source:org.apache.flume.source.NetcatSource{name:netsource,state:START} }} sinkRunners:{hdfssink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@22d7472e counterGroup:{ name:null counters:{runner.backoffs.consecutive=20, runner.backoffs=20} } }} channels:{memorychannel=org.apache.flume.channel.MemoryChannel{name: memorychannel}} }
10 Dec 2014 04:46:26,392 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.stopAllComponents:105)  - Stopping Source netsource
10 Dec 2014 04:46:26,392 INFO  [conf-file-poller-0] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:171)  - Stopping component: EventDrivenSourceRunner: { source:org.apache.flume.source.NetcatSource{name:netsource,state:START} }
10 Dec 2014 04:46:26,392 INFO  [conf-file-poller-0] (org.apache.flume.source.NetcatSource.stop:190)  - Source stopping
10 Dec 2014 04:46:26,392 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.stopAllComponents:115)  - Stopping Sink hdfssink
10 Dec 2014 04:46:26,393 INFO  [conf-file-poller-0] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:171)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@22d7472e counterGroup:{ name:null counters:{runner.backoffs.consecutive=20, runner.backoffs=20} } }
10 Dec 2014 04:46:26,393 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfssink stopped
10 Dec 2014 04:46:26,393 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfssink. sink.start.time == 1418166836375
10 Dec 2014 04:46:26,393 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfssink. sink.stop.time == 1418166986393
10 Dec 2014 04:46:26,393 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.complete == 0
10 Dec 2014 04:46:26,393 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.empty == 20
10 Dec 2014 04:46:26,393 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.underflow == 0
10 Dec 2014 04:46:26,393 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.closed.count == 0
10 Dec 2014 04:46:26,393 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.creation.count == 0
10 Dec 2014 04:46:26,394 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.failed.count == 0
10 Dec 2014 04:46:26,394 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.event.drain.attempt == 0
10 Dec 2014 04:46:26,394 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.event.drain.sucess == 0
10 Dec 2014 04:46:26,394 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.stopAllComponents:125)  - Stopping Channel memorychannel
10 Dec 2014 04:46:26,394 INFO  [conf-file-poller-0] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:171)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: memorychannel}
10 Dec 2014 04:46:26,394 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memorychannel stopped
10 Dec 2014 04:46:26,395 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.start.time == 1418166836371
10 Dec 2014 04:46:26,395 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.stop.time == 1418166986394
10 Dec 2014 04:46:26,395 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.capacity == 1000
10 Dec 2014 04:46:26,395 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.current.size == 0
10 Dec 2014 04:46:26,395 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.put.attempt == 0
10 Dec 2014 04:46:26,395 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.put.success == 0
10 Dec 2014 04:46:26,395 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.take.attempt == 20
10 Dec 2014 04:46:26,396 INFO  [conf-file-poller-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.take.success == 0
10 Dec 2014 04:46:26,396 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{netsource=EventDrivenSourceRunner: { source:org.apache.flume.source.NetcatSource{name:netsource,state:IDLE} }} sinkRunners:{hdfssink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@269f046 counterGroup:{ name:null counters:{} } }} channels:{memorychannel=org.apache.flume.channel.MemoryChannel{name: memorychannel}} }
10 Dec 2014 04:46:26,396 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memorychannel
10 Dec 2014 04:46:26,396 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memorychannel started
10 Dec 2014 04:46:26,396 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfssink
10 Dec 2014 04:46:26,397 ERROR [lifecycleSupervisor-1-7] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:113)  - Failed to register monitored counter group for type: SINK, name: hdfssink
javax.management.InstanceAlreadyExistsException: org.apache.flume.sink:type=hdfssink
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:108)
	at org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:88)
	at org.apache.flume.sink.hdfs.HDFSEventSink.start(HDFSEventSink.java:484)
	at org.apache.flume.sink.DefaultSinkProcessor.start(DefaultSinkProcessor.java:46)
	at org.apache.flume.SinkRunner.start(SinkRunner.java:79)
	at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:251)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
10 Dec 2014 04:46:26,397 INFO  [lifecycleSupervisor-1-7] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfssink started
10 Dec 2014 04:46:26,397 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source netsource
10 Dec 2014 04:46:26,399 INFO  [lifecycleSupervisor-1-8] (org.apache.flume.source.NetcatSource.start:150)  - Source starting
10 Dec 2014 04:46:26,399 INFO  [lifecycleSupervisor-1-8] (org.apache.flume.source.NetcatSource.start:164)  - Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:3000]
10 Dec 2014 04:57:16,466 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:79)  - Stopping lifecycle supervisor 10
10 Dec 2014 04:57:16,469 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfssink stopped
10 Dec 2014 04:57:16,469 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfssink. sink.start.time == 1418166986397
10 Dec 2014 04:57:16,469 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfssink. sink.stop.time == 1418167636469
10 Dec 2014 04:57:16,469 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.complete == 0
10 Dec 2014 04:57:16,469 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.empty == 83
10 Dec 2014 04:57:16,469 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.underflow == 0
10 Dec 2014 04:57:16,469 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.closed.count == 0
10 Dec 2014 04:57:16,469 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.creation.count == 0
10 Dec 2014 04:57:16,470 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.failed.count == 0
10 Dec 2014 04:57:16,470 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.event.drain.attempt == 0
10 Dec 2014 04:57:16,470 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.event.drain.sucess == 0
10 Dec 2014 04:57:16,470 INFO  [agent-shutdown-hook] (org.apache.flume.source.NetcatSource.stop:190)  - Source stopping
10 Dec 2014 04:57:16,470 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:83)  - Configuration provider stopping
10 Dec 2014 04:57:16,470 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memorychannel stopped
10 Dec 2014 04:57:16,470 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.start.time == 1418166986396
10 Dec 2014 04:57:16,471 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.stop.time == 1418167636470
10 Dec 2014 04:57:16,471 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.capacity == 1000
10 Dec 2014 04:57:16,471 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.current.size == 0
10 Dec 2014 04:57:16,471 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.put.attempt == 0
10 Dec 2014 04:57:16,471 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.put.success == 0
10 Dec 2014 04:57:16,471 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.take.attempt == 83
10 Dec 2014 04:57:16,471 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.take.success == 0
10 Dec 2014 04:57:20,885 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
10 Dec 2014 04:57:20,890 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
10 Dec 2014 04:57:20,895 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:57:20,896 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfssink Agent: agent
10 Dec 2014 04:57:20,896 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:57:20,896 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:57:20,896 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:57:20,896 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:57:20,897 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:57:20,897 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
10 Dec 2014 04:57:20,914 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
10 Dec 2014 04:57:20,914 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
10 Dec 2014 04:57:20,930 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memorychannel type memory
10 Dec 2014 04:57:20,937 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memorychannel
10 Dec 2014 04:57:20,938 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source netsource, type netcat
10 Dec 2014 04:57:20,985 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfssink, type: hdfs
10 Dec 2014 04:57:21,408 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
10 Dec 2014 04:57:21,787 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
10 Dec 2014 04:57:21,789 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memorychannel connected to [netsource, hdfssink]
10 Dec 2014 04:57:21,815 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{netsource=EventDrivenSourceRunner: { source:org.apache.flume.source.NetcatSource{name:netsource,state:IDLE} }} sinkRunners:{hdfssink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@603a669a counterGroup:{ name:null counters:{} } }} channels:{memorychannel=org.apache.flume.channel.MemoryChannel{name: memorychannel}} }
10 Dec 2014 04:57:21,815 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memorychannel
10 Dec 2014 04:57:21,873 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memorychannel, registered successfully.
10 Dec 2014 04:57:21,873 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memorychannel started
10 Dec 2014 04:57:21,873 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfssink
10 Dec 2014 04:57:21,874 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source netsource
10 Dec 2014 04:57:21,874 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.NetcatSource.start:150)  - Source starting
10 Dec 2014 04:57:21,878 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfssink, registered successfully.
10 Dec 2014 04:57:21,879 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfssink started
10 Dec 2014 04:57:21,901 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.NetcatSource.start:164)  - Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:3000]
10 Dec 2014 04:57:46,638 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:56)  - Serializer = TEXT, UseRawLocalFileSystem = false
10 Dec 2014 04:57:46,768 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666639.tmp
10 Dec 2014 04:57:52,974 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.append:400)  - Block Under-replication detected. Rotating file.
10 Dec 2014 04:57:52,999 INFO  [hdfs-hdfssink-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666639.tmp to hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666639
10 Dec 2014 04:57:53,067 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666640.tmp
10 Dec 2014 04:57:57,911 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.append:400)  - Block Under-replication detected. Rotating file.
10 Dec 2014 04:57:57,926 INFO  [hdfs-hdfssink-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666640.tmp to hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666640
10 Dec 2014 04:57:57,966 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666641.tmp
10 Dec 2014 04:58:03,246 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.append:400)  - Block Under-replication detected. Rotating file.
10 Dec 2014 04:58:03,260 INFO  [hdfs-hdfssink-call-runner-5] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666641.tmp to hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666641
10 Dec 2014 04:58:03,291 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666642.tmp
10 Dec 2014 04:58:11,445 INFO  [hdfs-hdfssink-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666642.tmp to hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666642
10 Dec 2014 04:58:11,482 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666643.tmp
10 Dec 2014 04:58:16,951 INFO  [hdfs-hdfssink-call-runner-3] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666643.tmp to hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666643
10 Dec 2014 04:58:16,984 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666644.tmp
10 Dec 2014 04:58:45,818 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.append:400)  - Block Under-replication detected. Rotating file.
10 Dec 2014 04:58:45,840 INFO  [hdfs-hdfssink-call-runner-1] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666644.tmp to hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666644
10 Dec 2014 04:58:45,872 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flume_network/2014/12/10/log.1418167666645.tmp
11 Dec 2014 06:34:10,933 WARN  [ResponseProcessor for block BP-962911770-192.168.1.8-1417287680178:blk_1073742182_1374] (org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run:874)  - DFSOutputStream ResponseProcessor exception  for block BP-962911770-192.168.1.8-1417287680178:blk_1073742182_1374
java.io.IOException: Bad response ERROR for block BP-962911770-192.168.1.8-1417287680178:blk_1073742182_1374 from datanode 192.168.1.8:50010
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:819)
11 Dec 2014 06:34:10,937 WARN  [DataStreamer for file /flume_network/2014/12/10/log.1418167666645.tmp block BP-962911770-192.168.1.8-1417287680178:blk_1073742182_1374] (org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery:1133)  - Error Recovery for block BP-962911770-192.168.1.8-1417287680178:blk_1073742182_1374 in pipeline 192.168.1.9:50010, 192.168.1.8:50010: bad datanode 192.168.1.8:50010
11 Dec 2014 06:34:43,892 WARN  [ResponseProcessor for block BP-962911770-192.168.1.8-1417287680178:blk_1073742182_1375] (org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run:874)  - DFSOutputStream ResponseProcessor exception  for block BP-962911770-192.168.1.8-1417287680178:blk_1073742182_1375
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:1986)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:176)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:796)
11 Dec 2014 06:34:43,895 WARN  [DataStreamer for file /flume_network/2014/12/10/log.1418167666645.tmp block BP-962911770-192.168.1.8-1417287680178:blk_1073742182_1375] (org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery:1133)  - Error Recovery for block BP-962911770-192.168.1.8-1417287680178:blk_1073742182_1375 in pipeline 192.168.1.9:50010, 192.168.1.10:50010: bad datanode 192.168.1.9:50010
11 Dec 2014 06:34:43,899 WARN  [DataStreamer for file /flume_network/2014/12/10/log.1418167666645.tmp block BP-962911770-192.168.1.8-1417287680178:blk_1073742182_1375] (org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run:627)  - DataStreamer Exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[192.168.1.10:50010], original=[192.168.1.10:50010]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:960)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1026)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1175)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:924)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:486)
11 Dec 2014 06:39:15,056 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn2.hadoop.com/192.168.1.9:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 06:39:15,057 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 30 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 06:42:11,900 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn1.hadoop.com/192.168.1.8:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 06:42:11,902 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 183 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 06:44:45,811 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn2.hadoop.com/192.168.1.9:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 06:44:45,813 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 360 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 06:47:50,785 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn1.hadoop.com/192.168.1.8:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 06:47:50,786 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 514 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 06:50:45,124 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn2.hadoop.com/192.168.1.9:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 06:50:45,124 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 699 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 06:52:56,155 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn1.hadoop.com/192.168.1.8:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 06:52:56,155 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 873 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 06:55:52,913 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn2.hadoop.com/192.168.1.9:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 06:55:52,914 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 1004 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 06:58:58,755 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn1.hadoop.com/192.168.1.8:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 06:58:58,756 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 1181 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:01:35,252 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn2.hadoop.com/192.168.1.9:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:01:35,254 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 1367 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:04:34,046 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn1.hadoop.com/192.168.1.8:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:04:34,046 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 1523 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:07:21,465 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn2.hadoop.com/192.168.1.9:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:07:21,465 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 1702 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:09:48,462 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn1.hadoop.com/192.168.1.8:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:09:48,462 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 1869 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:12:45,816 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn2.hadoop.com/192.168.1.9:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:12:45,816 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 2016 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:15:21,818 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn1.hadoop.com/192.168.1.8:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:15:21,818 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 2194 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:18:08,184 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn2.hadoop.com/192.168.1.9:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:18:08,184 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 2350 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:21:05,697 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn1.hadoop.com/192.168.1.8:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:21:05,698 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 2516 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:24:03,152 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn2.hadoop.com/192.168.1.9:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:24:03,152 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 2694 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:27:00,263 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn1.hadoop.com/192.168.1.8:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:27:00,264 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 2871 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:29:17,815 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn2.hadoop.com/192.168.1.9:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:29:17,816 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 3048 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:32:20,838 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn1.hadoop.com/192.168.1.8:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:32:20,839 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 3186 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:35:03,018 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn2.hadoop.com/192.168.1.9:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:35:03,018 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.LeaseRenewer.run:458)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1071136012_27] for 3369 seconds.  Will retry shortly ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn2.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:37:39,934 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:119)  - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease over nn1.hadoop.com/192.168.1.8:8020. Not retrying because failovers (15) exceeded maximum allowed (15)
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
11 Dec 2014 07:37:39,935 WARN  [LeaseRenewer:hdfs@hacluster:8020] (org.apache.hadoop.hdfs.DFSClient.renewLease:798)  - Failed to renew lease for DFSClient_NONMAPREDUCE_-1071136012_27 for 3687 seconds (>= hard-limit =3600 seconds.) Closing all files being written ...
java.net.ConnectException: Call From dn1.hadoop.com/192.168.1.10 to nn1.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1414)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:532)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy17.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:791)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:604)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 16 more
12 Dec 2014 03:43:39,902 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
12 Dec 2014 03:43:39,907 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
12 Dec 2014 03:43:39,913 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 03:43:39,913 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfssink Agent: agent
12 Dec 2014 03:43:39,914 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 03:43:39,914 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 03:43:39,914 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 03:43:39,914 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 03:43:39,914 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 03:43:39,914 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 03:43:39,933 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
12 Dec 2014 03:43:39,933 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
12 Dec 2014 03:43:39,947 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memorychannel type memory
12 Dec 2014 03:43:39,954 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memorychannel
12 Dec 2014 03:43:39,955 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source netsource, type netcat
12 Dec 2014 03:43:40,004 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfssink, type: hdfs
12 Dec 2014 03:43:40,411 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
12 Dec 2014 03:43:40,743 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
12 Dec 2014 03:43:40,745 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memorychannel connected to [netsource, hdfssink]
12 Dec 2014 03:43:40,776 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{netsource=EventDrivenSourceRunner: { source:org.apache.flume.source.NetcatSource{name:netsource,state:IDLE} }} sinkRunners:{hdfssink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@1a2639d1 counterGroup:{ name:null counters:{} } }} channels:{memorychannel=org.apache.flume.channel.MemoryChannel{name: memorychannel}} }
12 Dec 2014 03:43:40,777 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memorychannel
12 Dec 2014 03:43:40,875 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memorychannel, registered successfully.
12 Dec 2014 03:43:40,875 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memorychannel started
12 Dec 2014 03:43:40,878 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfssink
12 Dec 2014 03:43:40,879 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source netsource
12 Dec 2014 03:43:40,879 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.NetcatSource.start:150)  - Source starting
12 Dec 2014 03:43:40,887 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfssink, registered successfully.
12 Dec 2014 03:43:40,887 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfssink started
12 Dec 2014 03:43:40,914 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.NetcatSource.start:164)  - Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:3000]
12 Dec 2014 04:02:29,376 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
12 Dec 2014 04:02:29,383 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
12 Dec 2014 04:02:29,388 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 04:02:29,388 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfssink Agent: agent
12 Dec 2014 04:02:29,388 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 04:02:29,388 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 04:02:29,388 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 04:02:29,388 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 04:02:29,388 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 04:02:29,388 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 04:02:29,407 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
12 Dec 2014 04:02:29,407 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
12 Dec 2014 04:02:29,420 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memorychannel type memory
12 Dec 2014 04:02:29,431 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memorychannel
12 Dec 2014 04:02:29,432 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source netsource, type netcat
12 Dec 2014 04:02:29,478 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfssink, type: hdfs
12 Dec 2014 04:02:29,913 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
12 Dec 2014 04:02:30,278 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
12 Dec 2014 04:02:30,280 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memorychannel connected to [netsource, hdfssink]
12 Dec 2014 04:02:30,308 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{netsource=EventDrivenSourceRunner: { source:org.apache.flume.source.NetcatSource{name:netsource,state:IDLE} }} sinkRunners:{hdfssink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@603a669a counterGroup:{ name:null counters:{} } }} channels:{memorychannel=org.apache.flume.channel.MemoryChannel{name: memorychannel}} }
12 Dec 2014 04:02:30,308 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memorychannel
12 Dec 2014 04:02:30,380 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memorychannel, registered successfully.
12 Dec 2014 04:02:30,380 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memorychannel started
12 Dec 2014 04:02:30,381 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfssink
12 Dec 2014 04:02:30,381 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source netsource
12 Dec 2014 04:02:30,381 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.NetcatSource.start:150)  - Source starting
12 Dec 2014 04:02:30,391 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfssink, registered successfully.
12 Dec 2014 04:02:30,392 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfssink started
12 Dec 2014 04:02:30,410 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.NetcatSource.start:164)  - Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:3000]
12 Dec 2014 04:02:51,440 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:79)  - Stopping lifecycle supervisor 10
12 Dec 2014 04:02:51,444 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfssink stopped
12 Dec 2014 04:02:51,445 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfssink. sink.start.time == 1418337150391
12 Dec 2014 04:02:51,445 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfssink. sink.stop.time == 1418337171444
12 Dec 2014 04:02:51,445 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.complete == 0
12 Dec 2014 04:02:51,445 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.empty == 4
12 Dec 2014 04:02:51,445 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.underflow == 0
12 Dec 2014 04:02:51,445 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.closed.count == 0
12 Dec 2014 04:02:51,445 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.creation.count == 0
12 Dec 2014 04:02:51,445 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.failed.count == 0
12 Dec 2014 04:02:51,445 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.event.drain.attempt == 0
12 Dec 2014 04:02:51,445 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.event.drain.sucess == 0
12 Dec 2014 04:02:51,446 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memorychannel stopped
12 Dec 2014 04:02:51,446 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.start.time == 1418337150380
12 Dec 2014 04:02:51,446 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.stop.time == 1418337171446
12 Dec 2014 04:02:51,446 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.capacity == 1000
12 Dec 2014 04:02:51,447 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.current.size == 0
12 Dec 2014 04:02:51,447 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.put.attempt == 0
12 Dec 2014 04:02:51,447 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.put.success == 0
12 Dec 2014 04:02:51,447 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.take.attempt == 4
12 Dec 2014 04:02:51,447 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.take.success == 0
12 Dec 2014 04:02:51,447 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:83)  - Configuration provider stopping
12 Dec 2014 04:02:51,447 INFO  [agent-shutdown-hook] (org.apache.flume.source.NetcatSource.stop:190)  - Source stopping
12 Dec 2014 04:07:55,600 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
12 Dec 2014 04:07:55,605 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
12 Dec 2014 04:07:55,610 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 04:07:55,610 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfssink Agent: agent
12 Dec 2014 04:07:55,610 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 04:07:55,611 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 04:07:55,611 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 04:07:55,611 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 04:07:55,611 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 04:07:55,611 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
12 Dec 2014 04:07:55,628 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
12 Dec 2014 04:07:55,628 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
12 Dec 2014 04:07:55,641 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memorychannel type memory
12 Dec 2014 04:07:55,649 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memorychannel
12 Dec 2014 04:07:55,652 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source netsource, type netcat
12 Dec 2014 04:07:55,700 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfssink, type: hdfs
12 Dec 2014 04:07:56,142 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
12 Dec 2014 04:07:56,480 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
12 Dec 2014 04:07:56,483 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memorychannel connected to [netsource, hdfssink]
12 Dec 2014 04:07:56,509 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{netsource=EventDrivenSourceRunner: { source:org.apache.flume.source.NetcatSource{name:netsource,state:IDLE} }} sinkRunners:{hdfssink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@603a669a counterGroup:{ name:null counters:{} } }} channels:{memorychannel=org.apache.flume.channel.MemoryChannel{name: memorychannel}} }
12 Dec 2014 04:07:56,510 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memorychannel
12 Dec 2014 04:07:56,596 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memorychannel, registered successfully.
12 Dec 2014 04:07:56,596 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memorychannel started
12 Dec 2014 04:07:56,596 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfssink
12 Dec 2014 04:07:56,597 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source netsource
12 Dec 2014 04:07:56,597 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.NetcatSource.start:150)  - Source starting
12 Dec 2014 04:07:56,606 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfssink, registered successfully.
12 Dec 2014 04:07:56,606 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfssink started
12 Dec 2014 04:07:56,626 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.NetcatSource.start:164)  - Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:3000]
12 Dec 2014 15:41:20,229 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:79)  - Stopping lifecycle supervisor 10
12 Dec 2014 15:41:20,235 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfssink stopped
12 Dec 2014 15:41:20,235 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfssink. sink.start.time == 1418337476606
12 Dec 2014 15:41:20,235 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfssink. sink.stop.time == 1418379080235
12 Dec 2014 15:41:20,236 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.complete == 0
12 Dec 2014 15:41:20,236 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.empty == 5201
12 Dec 2014 15:41:20,236 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.underflow == 0
12 Dec 2014 15:41:20,236 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.closed.count == 0
12 Dec 2014 15:41:20,236 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.creation.count == 0
12 Dec 2014 15:41:20,236 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.failed.count == 0
12 Dec 2014 15:41:20,237 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.event.drain.attempt == 0
12 Dec 2014 15:41:20,237 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.event.drain.sucess == 0
12 Dec 2014 15:41:20,237 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memorychannel stopped
12 Dec 2014 15:41:20,237 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.start.time == 1418337476596
12 Dec 2014 15:41:20,237 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.stop.time == 1418379080237
12 Dec 2014 15:41:20,237 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.capacity == 1000
12 Dec 2014 15:41:20,238 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.current.size == 0
12 Dec 2014 15:41:20,238 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.put.attempt == 0
12 Dec 2014 15:41:20,238 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.put.success == 0
12 Dec 2014 15:41:20,238 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.take.attempt == 5201
12 Dec 2014 15:41:20,240 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.take.success == 0
12 Dec 2014 15:41:20,241 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:83)  - Configuration provider stopping
12 Dec 2014 15:41:20,244 INFO  [agent-shutdown-hook] (org.apache.flume.source.NetcatSource.stop:190)  - Source stopping
22 Feb 2015 06:19:36,029 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
22 Feb 2015 06:19:36,039 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
22 Feb 2015 06:19:36,045 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
22 Feb 2015 06:19:36,046 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfssink Agent: agent
22 Feb 2015 06:19:36,046 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
22 Feb 2015 06:19:36,046 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
22 Feb 2015 06:19:36,046 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
22 Feb 2015 06:19:36,046 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
22 Feb 2015 06:19:36,048 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
22 Feb 2015 06:19:36,048 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfssink
22 Feb 2015 06:19:36,080 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
22 Feb 2015 06:19:36,080 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
22 Feb 2015 06:19:36,104 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memorychannel type memory
22 Feb 2015 06:19:36,122 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memorychannel
22 Feb 2015 06:19:36,123 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source netsource, type netcat
22 Feb 2015 06:19:36,178 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfssink, type: hdfs
22 Feb 2015 06:19:36,570 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22 Feb 2015 06:19:36,966 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
22 Feb 2015 06:19:36,968 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memorychannel connected to [netsource, hdfssink]
22 Feb 2015 06:19:36,991 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{netsource=EventDrivenSourceRunner: { source:org.apache.flume.source.NetcatSource{name:netsource,state:IDLE} }} sinkRunners:{hdfssink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@181090c0 counterGroup:{ name:null counters:{} } }} channels:{memorychannel=org.apache.flume.channel.MemoryChannel{name: memorychannel}} }
22 Feb 2015 06:19:36,994 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memorychannel
22 Feb 2015 06:19:37,059 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memorychannel, registered successfully.
22 Feb 2015 06:19:37,060 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memorychannel started
22 Feb 2015 06:19:37,060 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfssink
22 Feb 2015 06:19:37,060 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source netsource
22 Feb 2015 06:19:37,061 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.NetcatSource.start:150)  - Source starting
22 Feb 2015 06:19:37,066 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfssink, registered successfully.
22 Feb 2015 06:19:37,066 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfssink started
22 Feb 2015 06:19:37,098 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.NetcatSource.start:164)  - Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:3000]
22 Feb 2015 06:20:08,609 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:56)  - Serializer = TEXT, UseRawLocalFileSystem = false
22 Feb 2015 06:20:08,788 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flume_network/2015/02/22/log.1424566208610.tmp
22 Feb 2015 06:29:27,301 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:79)  - Stopping lifecycle supervisor 10
22 Feb 2015 06:29:27,304 INFO  [agent-shutdown-hook] (org.apache.flume.source.NetcatSource.stop:190)  - Source stopping
22 Feb 2015 06:29:27,305 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:437)  - Closing hdfs://hacluster:8020/flume_network/2015/02/22/log
22 Feb 2015 06:29:27,372 INFO  [hdfs-hdfssink-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flume_network/2015/02/22/log.1424566208610.tmp to hdfs://hacluster:8020/flume_network/2015/02/22/log.1424566208610
22 Feb 2015 06:29:27,385 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfssink stopped
22 Feb 2015 06:29:27,385 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfssink. sink.start.time == 1424566177066
22 Feb 2015 06:29:27,386 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfssink. sink.stop.time == 1424566767385
22 Feb 2015 06:29:27,386 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.complete == 0
22 Feb 2015 06:29:27,386 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.empty == 75
22 Feb 2015 06:29:27,386 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.batch.underflow == 6
22 Feb 2015 06:29:27,386 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.closed.count == 1
22 Feb 2015 06:29:27,386 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.creation.count == 1
22 Feb 2015 06:29:27,386 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.connection.failed.count == 0
22 Feb 2015 06:29:27,386 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.event.drain.attempt == 8
22 Feb 2015 06:29:27,387 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfssink. sink.event.drain.sucess == 8
22 Feb 2015 06:29:27,387 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memorychannel stopped
22 Feb 2015 06:29:27,387 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.start.time == 1424566177060
22 Feb 2015 06:29:27,387 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.stop.time == 1424566767387
22 Feb 2015 06:29:27,387 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.capacity == 1000
22 Feb 2015 06:29:27,387 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.current.size == 0
22 Feb 2015 06:29:27,387 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.put.attempt == 8
22 Feb 2015 06:29:27,387 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.put.success == 8
22 Feb 2015 06:29:27,393 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.take.attempt == 89
22 Feb 2015 06:29:27,393 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memorychannel. channel.event.take.success == 8
22 Feb 2015 06:29:27,394 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:83)  - Configuration provider stopping
22 Feb 2015 06:29:33,659 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:61)  - Configuration provider starting
22 Feb 2015 06:29:33,668 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:133)  - Reloading configuration file:/apache/flume/conf/flume.conf
22 Feb 2015 06:29:33,679 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
22 Feb 2015 06:29:33,679 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
22 Feb 2015 06:29:33,679 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:930)  - Added sinks: hdfsSink Agent: agent
22 Feb 2015 06:29:33,679 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
22 Feb 2015 06:29:33,680 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
22 Feb 2015 06:29:33,680 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1016)  - Processing:hdfsSink
22 Feb 2015 06:29:33,713 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:140)  - Post-validation flume configuration contains configuration for agents: [agent]
22 Feb 2015 06:29:33,713 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:150)  - Creating channels
22 Feb 2015 06:29:33,735 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:40)  - Creating instance of channel memoryChannel type memory
22 Feb 2015 06:29:33,744 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel memoryChannel
22 Feb 2015 06:29:33,751 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:39)  - Creating instance of source logstream, type exec
22 Feb 2015 06:29:33,776 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:40)  - Creating instance of sink: hdfsSink, type: hdfs
22 Feb 2015 06:29:34,320 WARN  [conf-file-poller-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:62)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22 Feb 2015 06:29:34,837 INFO  [conf-file-poller-0] (org.apache.flume.sink.hdfs.HDFSEventSink.authenticate:493)  - Hadoop Security enabled: false
22 Feb 2015 06:29:34,850 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:119)  - Channel memoryChannel connected to [logstream, hdfsSink]
22 Feb 2015 06:29:34,869 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:138)  - Starting new configuration:{ sourceRunners:{logstream=EventDrivenSourceRunner: { source:org.apache.flume.source.ExecSource{name:logstream,state:IDLE} }} sinkRunners:{hdfsSink=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@54274d27 counterGroup:{ name:null counters:{} } }} channels:{memoryChannel=org.apache.flume.channel.MemoryChannel{name: memoryChannel}} }
22 Feb 2015 06:29:34,886 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:145)  - Starting Channel memoryChannel
22 Feb 2015 06:29:34,928 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: CHANNEL, name: memoryChannel, registered successfully.
22 Feb 2015 06:29:34,929 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: CHANNEL, name: memoryChannel started
22 Feb 2015 06:29:34,929 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:173)  - Starting Sink hdfsSink
22 Feb 2015 06:29:34,929 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:184)  - Starting Source logstream
22 Feb 2015 06:29:34,930 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.source.ExecSource.start:163)  - Exec source starting with command:tail -f /apache/flume/test
22 Feb 2015 06:29:34,933 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SINK, name: hdfsSink, registered successfully.
22 Feb 2015 06:29:34,933 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SINK, name: hdfsSink started
22 Feb 2015 06:29:34,936 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:110)  - Monitoried counter group for type: SOURCE, name: logstream, registered successfully.
22 Feb 2015 06:29:34,936 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:94)  - Component type: SOURCE, name: logstream started
22 Feb 2015 06:29:38,948 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSSequenceFile.configure:63)  - writeFormat = Text, UseRawLocalFileSystem = false
22 Feb 2015 06:29:39,089 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1424566778942.tmp
22 Feb 2015 06:30:11,260 INFO  [hdfs-hdfsSink-call-runner-3] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1424566778942.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1424566778942
22 Feb 2015 06:31:10,598 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1424566778943.tmp
22 Feb 2015 06:31:40,736 INFO  [hdfs-hdfsSink-call-runner-8] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1424566778943.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1424566778943
22 Feb 2015 06:32:07,762 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1424566778944.tmp
22 Feb 2015 06:32:37,855 INFO  [hdfs-hdfsSink-call-runner-5] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1424566778944.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1424566778944
22 Feb 2015 06:40:05,013 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:219)  - Creating hdfs://hacluster:8020/flumetest/FlumeData.1424566778945.tmp
22 Feb 2015 06:40:35,166 INFO  [hdfs-hdfsSink-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:487)  - Renaming hdfs://hacluster:8020/flumetest/FlumeData.1424566778945.tmp to hdfs://hacluster:8020/flumetest/FlumeData.1424566778945
22 Feb 2015 07:29:31,813 INFO  [pool-3-thread-1] (org.apache.flume.source.ExecSource$ExecRunnable.run:370)  - Command [tail -f /apache/flume/test] exited with 143
22 Feb 2015 07:29:31,838 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:79)  - Stopping lifecycle supervisor 10
22 Feb 2015 07:29:31,857 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: CHANNEL, name: memoryChannel stopped
22 Feb 2015 07:29:31,858 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.start.time == 1424566774929
22 Feb 2015 07:29:31,893 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.stop.time == 1424570371857
22 Feb 2015 07:29:31,899 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.capacity == 100
22 Feb 2015 07:29:31,899 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.current.size == 0
22 Feb 2015 07:29:31,899 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.attempt == 14
22 Feb 2015 07:29:31,907 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.put.success == 14
22 Feb 2015 07:29:31,907 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.attempt == 473
22 Feb 2015 07:29:31,908 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: CHANNEL, name: memoryChannel. channel.event.take.success == 14
22 Feb 2015 07:29:31,920 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:437)  - Closing hdfs://hacluster:8020/flumetest/FlumeData
22 Feb 2015 07:29:31,957 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.close:296)  - HDFSWriter is already closed: hdfs://hacluster:8020/flumetest/FlumeData.1424566778945.tmp
22 Feb 2015 07:29:31,973 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SINK, name: hdfsSink stopped
22 Feb 2015 07:29:31,976 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.start.time == 1424566774933
22 Feb 2015 07:29:31,976 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.stop.time == 1424570371973
22 Feb 2015 07:29:31,976 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.complete == 0
22 Feb 2015 07:29:31,976 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.empty == 454
22 Feb 2015 07:29:31,976 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.batch.underflow == 5
22 Feb 2015 07:29:31,977 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.closed.count == 4
22 Feb 2015 07:29:31,977 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.creation.count == 4
22 Feb 2015 07:29:31,977 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.connection.failed.count == 0
22 Feb 2015 07:29:31,977 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.attempt == 14
22 Feb 2015 07:29:31,977 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SINK, name: hdfsSink. sink.event.drain.sucess == 14
22 Feb 2015 07:29:32,000 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:83)  - Configuration provider stopping
22 Feb 2015 07:29:32,001 INFO  [agent-shutdown-hook] (org.apache.flume.source.ExecSource.stop:186)  - Stopping exec source with command:tail -f /apache/flume/test
22 Feb 2015 07:29:32,010 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:139)  - Component type: SOURCE, name: logstream stopped
22 Feb 2015 07:29:32,011 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:145)  - Shutdown Metric for type: SOURCE, name: logstream. source.start.time == 1424566774936
22 Feb 2015 07:29:32,011 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:151)  - Shutdown Metric for type: SOURCE, name: logstream. source.stop.time == 1424570372010
22 Feb 2015 07:29:32,011 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append-batch.accepted == 0
22 Feb 2015 07:29:32,011 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append-batch.received == 0
22 Feb 2015 07:29:32,011 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append.accepted == 0
22 Feb 2015 07:29:32,012 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.append.received == 0
22 Feb 2015 07:29:32,012 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.events.accepted == 14
22 Feb 2015 07:29:32,021 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.events.received == 14
22 Feb 2015 07:29:32,021 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:167)  - Shutdown Metric for type: SOURCE, name: logstream. src.open-connection.count == 0
